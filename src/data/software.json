[{
      "@id": "http://software.esciencecenter.nl/software/3d-e-chem-vm/",
      "id": "/software/3d-e-chem-vm",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "name": "3D-e-Chem Virtual machine",
      "endorsedBy": ["/organization/nlesc"],
      "tagLine": "Virtual machine with all software and sample data to run 3D-e-Chem Knime workflows",
      "codeRepository": "https://github.com/3D-e-Chem/3D-e-Chem-VM",
      "website": "https://3d-e-chem.github.io/3D-e-Chem-VM",
      "downloadUrl": "https://atlas.hashicorp.com/nlesc/boxes/3d-e-chem",
      "documentationUrl": "https://github.com/3D-e-Chem/3D-e-Chem-VM/wiki",
      "doi": "http://dx.doi.org/10.5281/zenodo.51474",
      "programmingLanguage": ["YAML"],
      "license": ["apache-2.0"],
      "competence": ["Optimized Data Handling"],
      "discipline": ["eScience Methodology"],
      "expertise": ["Reproducible Research"],
      "contactPerson": "/person/s.verhoeven",
      "owner": ["/organization/nlesc","/organization/radboud.university.nijmegen","/organization/vua"],
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/s.verhoeven",{"name":"Ross McGuire","affiliation":["/organization/radboud.university.nijmegen"],"linkedInUrl":"https://nl.linkedin.com/in/ross-mcguire-71457523"}],
      "involvedOrganization": ["/organization/nlesc","/organization/radboud.university.nijmegen","/organization/vua"],
      "usedIn": ["/project/3d-e-chem"],
      "startDate": "2016-01-11",
      "status": "active",
      "technologyTag": ["Knime","Vagrant","Packer"],
      "dependency": ["/software/chemical-analytics-platform"],
      "title": "3d E Chem Vm",
      "slug": "3d-e-chem-vm",
      
      "description": "<p><img src=\"https://3d-e-chem.github.io/3D-e-Chem-VM/assets/images/3d-e-chem-vm-screenshot.png\" alt=\"Screenshot of Desktop\" title=\"Screenshot\" /></p>\n\n<p>3D-e-chem VM is freely available Virtual Machine (VM) encompassing tools, databases &amp; workflows, including new resources developed for ligand binding site comparisons and GPCR research.</p>\n\n<p>The VM contains a fully functional cheminformatics infrastructure consisting of a chemistry enabled relational database system (PostgreSQL + RDKit) with a data analytics workflow tool (KNIME) and additional cheminformatics capabilities.</p>\n\n<p>Tools, workflows and reference data sets are made available. The wide range of cheminformatics functionalities are provided in the downloadable 3D-e-chem VM allowing immediate use in research and education.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/ahn2webviewer/",
      "id": "/software/ahn2webviewer",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/NLeSC/ahn-pointcloud-viewer",
      "website": "http://ahn2.pointclouds.nl/",
      "competence": ["Big Data Analytics"],
      "contactPerson": "/person/m.vanmeersbergen",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/m.vanmeersbergen","/person/o.rubi","/person/s.verhoeven"],
      "discipline": ["eScience Methodology"],
      "expertise": ["Scientific Visualization"],
      "involvedOrganization": ["/organization/nlesc"],
      "license": ["apache-2.0"],
      "dependency": ["/software/potree","/software/potreeconverter","/software/massivepotreeconverter"],
      "name": "AHN2 pointcloud viewer",
      "endorsedBy": ["/organization/nlesc"],
      "programmingLanguage": ["JavaScript"],
      "startDate": "2014-04-01",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "WebGL point cloud visualization of AHN2",
      "user": ["/organization/nlesc","/person/o.rubi"],
      "usedIn": ["/project/massive-point-clouds-for-esciences"],
      "owner": ["/organization/nlesc"],
      "technologyTag": ["Point clouds","WebGL","Website","3D"],
      "title": "Ahn2webviewer",
      "slug": "ahn2webviewer",
      
      "description": "<p>WebGL point cloud visualization of the Actuele Hoogtekaart Nederland 2. \nThis renderer is based on http://potree.org</p>\n\n<p>In order to visualize such a massive data set, the AHN2 has to be reorganized in a multi-resolution octree. This processing can be done with the Massive-PotreeConverter (&lt;/software/massivepotreeconverter&gt;) which is a extension of the PotreeConverter (&lt;/software/potreeconverter&gt;) to distribute the processing into multiple machines/cores.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/amuse/",
      "id": "/software/amuse",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/amusecode/amuse",
      "competence": ["Efficient Computing"],
      "contactPerson": "/person/n.drost",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/n.drost"],
      "discipline": ["Physics & Beyond"],
      "documentationUrl": "http://amusecode.org/doc/",
      "expertise": ["Distributed Computing","Accelerated Computing","High Performance Computing"],
      "endorsedBy": ["/organization/nlesc"],
      "license": ["gpl-2.0"],
      "logo": "/images/software/amuse.png",
      "name": "AMUSE",
      "owner": ["/organization/leiden-university"],
      "programmingLanguage": ["Python","Java","C++","C","FORTRAN","CUDA","OpenCL"],
      "status": "active",
      "tagLine": "The Astrophysical Multipurpose Simulation Environment",
      "technologyTag": ["Simulation","MultiModel"],
      "usedIn": ["/project/amuse","/project/abcmuse"],
      "user": ["/organization/nlesc"],
      "website": "http://amusecode.org/",
      "title": "Amuse",
      "slug": "amuse",
      
      "description": "<p>AMUSE is the Astrophysical Multipurpose Software Environment.</p>\n\n<p>Our aim is to provide a software framework for astrophysical simulations, in which existing codes from different domains, such as stellar dynamics, stellar evolution, hydrodynamics and radiative transfer can be easily coupled.</p>\n\n<p>AMUSE is a community effort with the main development by the AMUSE team at Leiden Observatory under supervision of Simon Portegies Zwart.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/cclustera/",
      "id": "/software/cclustera",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "name": "CClusTera",
      "endorsedBy": ["/organization/nlesc"],
      "tagLine": "A 3D web tool for interactive visualization of hierarchically clustered big data",
      "codeRepository": "http://github.com/NLeSC/CClusTera",
      "programmingLanguage": ["JavaScript"],
      "license": ["afl-3.0"],
      "competence": ["Big Data Analytics"],
      "discipline": ["eScience Methodology"],
      "expertise": ["Information Visualization"],
      "supportLevel": "specialized",
      "contactPerson": "/person/s.georgievska",
      "owner": ["/organization/nlesc"],
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/s.georgievska"],
      "user": ["/organization/nlesc"],
      "involvedOrganization": ["/organization/nlesc"],
      "usedIn": ["/project/massive-biological-data-clustering-reporting-and-visualization-tools"],
      "startDate": "2015-05-01",
      "status": "active",
      "technologyTag": ["3D graphics","Website"],
      "title": "Cclustera",
      "slug": "cclustera",
      
      "description": "<p>CClusTera enables interactive and web based 3D visualization of hierarchically clustered data. The application also enables colorizing points based on features other than their clusters.</p>\n\n<h1 id=\"why-cclustera\">Why CClusTera?</h1>\n\n<p>In many scientific domains data is hierarchically clustered based on similarity, primarily because of the large volume of data. A scientist would be then interested to view the clusters by expanding and collapsing the clusters, by loading data  up to a certain level in the hierarchy, or by colorizing the data based on features other than their clusters. This can provide new insights for domain research.</p>\n\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/cesium-ncwms/",
      "id": "/software/cesium-ncwms",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/NLeSC/Cesium-NcWMS",
      "competence": ["Big Data Analytics"],
      "contactPerson": "/person/m.vanmeersbergen",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/n.drost","/person/m.vanmeersbergen"],
      "discipline": ["Environment & Sustainability"],
      "expertise": ["Scientific Visualization"],
      "license": ["apache-2.0"],
      "name": "Cesium-ncWMS",
      "endorsedBy": ["/organization/nlesc"],
      "owner": ["/organization/nlesc"],
      "programmingLanguage": ["JavaScript","Java"],
      "startDate": "2014-10-08",
      "status": "active",
      "tagLine": "3D Globe Visualization of NetCDF data.",
      "technologyTag": ["Visualization","WebGL","3D"],
      "usedIn": ["/project/ewatercycle"],
      "user": ["/person/n.drost"],
      "title": "Cesium Ncwms",
      "slug": "cesium-ncwms",
      
      "description": "<p>Cesium (cesiumjs.org) based visualization using ncWMS to serve NetCDF data and D3 (d3js.org) to display graphs.\nA live running version of this software can be found at http://forecast.ewatercycle.org</p>\n\n<p><img src=\"https://github.com/NLeSC/Cesium-NcWMS/raw/master/DOC/images/ewa-saturation.png\" alt=\"logo\" title=\"Screenshot 1\" /></p>\n\n<p><img src=\"https://github.com/NLeSC/Cesium-NcWMS/raw/master/DOC/images/ewa-discharge.png\" alt=\"logo\" title=\"Screenshot 2\" /></p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/chemical-analytics-platform/",
      "id": "/software/chemical-analytics-platform",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "name": "Chemical Analytics Virtual Machine",
      "endorsedBy": ["/organization/nlesc"],
      "tagLine": "Packer template to create Vagrant box with Knime inside",
      "codeRepository": "https://github.com/NLeSC/Chemical-Analytics-Platform",
      "website": "https://github.com/NLeSC/Chemical-Analytics-Platform",
      "downloadUrl": "https://atlas.hashicorp.com/nlesc/boxes/chemical-analytics-platform",
      "documentationUrl": "https://github.com/NLeSC/Chemical-Analytics-Platform/wiki",
      "programmingLanguage": ["YAML"],
      "license": ["apache-2.0"],
      "competence": ["Optimized Data Handling"],
      "discipline": ["eScience Methodology"],
      "expertise": ["Reproducible Research"],
      "supportLevel": "specialized",
      "contactPerson": "/person/s.verhoeven",
      "owner": ["/organization/nlesc","/organization/radboud.university.nijmegen","/organization/vua"],
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/s.verhoeven",{"name":"Ross McGuire","affiliation":["/organization/radboud.university.nijmegen"],"linkedInUrl":"https://nl.linkedin.com/in/ross-mcguire-71457523"}],
      "involvedOrganization": ["/organization/nlesc","/organization/radboud.university.nijmegen","/organization/vua"],
      "usedIn": ["/project/3d-e-chem"],
      "startDate": "2015-08-05",
      "status": "active",
      "technologyTag": ["Knime","Vagrant","Packer"],
      "dependencyOf": ["/software/3d-e-chem-vm"],
      "title": "Chemical Analytics Platform",
      "slug": "chemical-analytics-platform",
      
      "description": "<p>Scripts to create a Vagrant box using packer and ansible.</p>\n\n<p>Vagrant box is a Virtual Machine in Virtualbox format.</p>\n\n<p>Start virtual machine with</p>\n\n<div class=\"highlighter-rouge\"><pre class=\"highlight\"><code>vagrant init nlesc/chemical-analytics-platform\nvagrant up\n</code></pre>\n</div>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/common-sense/",
      "id": "/software/common-sense",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "name": "Common Sense",
      "endorsedBy": ["/organization/nlesc"],
      "tagLine": "User-friendly web application for showing (GIS) data on a map.",
      "codeRepository": "https://github.com/TNOCS/csWeb/",
      "website": "https://tnocs.github.io/csWeb/",
      "documentationUrl": "https://github.com/TNOCS/csWeb/wiki/Getting-started",
      "logo": "/images/software/common-sense.png",
      "programmingLanguage": ["JavaScript","TypeScript"],
      "license": ["mit"],
      "competence": ["Big Data Analytics"],
      "discipline": ["eScience Methodology"],
      "expertise": ["Information Visualization"],
      "supportLevel": "specialized",
      "contactPerson": "/person/c.martinez",
      "owner": ["/organization/tno"],
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/c.martinez","/person/b.weel","/person/j.borgdorff",{"name":"Erik Vullings","githubUrl":"https://github.com/erikvullings","affiliation":["/organization/tno"]}],
      "user": ["/organization/nlesc","/organization/tno"],
      "involvedOrganization": ["/organization/nlesc","/organization/tno"],
      "usedIn": ["/project/simcity"],
      "startDate": "2014-09-11",
      "status": "active",
      "dependency": null,
      "dependencyOf": null,
      "technologyTag": ["GIS"],
      "title": "Common Sense",
      "slug": "common-sense",
      
      "description": "<p>csWeb, or the Common Sense Web application, is an intuitive open source web-based GIS application, providing casual users as well as business analysists and information manageners with a powerful tool to perform spatial analysis. It has a strong focus on usability and connectivity, be it connecting and sharing information with other users or connecting to services or calculation simulations and models</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/couchdb/",
      "id": "/software/couchdb",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/apache/couchdb",
      "competence": ["Optimized Data Handling"],
      "contactPerson": "/person/j.borgdorff",
      "dependency": ["/software/picas"],
      "dependencyOf": ["/software/docker-couch-admin","/software/picas"],
      "discipline": ["eScience Methodology"],
      "documentationUrl": "http://docs.couchdb.org/en/stable/",
      "downloadUrl": "http://couchdb.apache.org/#download",
      "expertise": ["Databases"],
      "license": ["apache-2.0"],
      "logo": "/images/software/couchdb.png",
      "name": "Apache CouchDB™",
      "programmingLanguage": ["JavaScript","Python","Ruby","Erlang"],
      "startDate": "2008-03-28,",
      "status": "active",
      "supportLevel": "basic",
      "tagLine": "Apache CouchDB™ is a database that uses JSON for documents, JavaScript for MapReduce indexes, and regular HTTP for its API.",
      "technologyTag": ["NoSQL database","DBMS"],
      "usedIn": ["/project/simcity"],
      "user": ["/person/j.borgdorff","/person/b.weel"],
      "website": "http://couchdb.apache.org",
      "title": "Couchdb",
      "slug": "couchdb",
      
      "description": "<p>CouchDB is a database that completely embraces the web. Store your data with\nJSON documents. Access your documents and query your indexes with your web\nbrowser, via HTTP. Index, combine, and transform your documents with\nJavaScript. CouchDB works well with modern web and mobile apps. You can even\nserve web apps directly out of CouchDB. And you can distribute your data, or\nyour apps, efficiently using CouchDB’s incremental replication. CouchDB\nsupports master-master setups with automatic conflict detection.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/cptm/",
      "id": "/software/cptm",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "name": "Cross-perspective Topic Modeling",
      "endorsedBy": ["/organization/nlesc"],
      "tagLine": "A Gibbs sampler that implements Cross-Perspective Topic Modeling",
      "codeRepository": "https://github.com/NLeSC/cptm",
      "nlescWebsite": null,
      "website": null,
      "documentationUrl": null,
      "logo": null,
      "doi": "http://dx.doi.org/10.5281/zenodo.47756",
      "programmingLanguage": ["Python","Cython"],
      "license": ["apache-2.0"],
      "competence": ["Big Data Analytics"],
      "discipline": ["Humanities & Social Sciences"],
      "expertise": ["Text Mining"],
      "supportLevel": "specialized",
      "contactPerson": "/person/j.vanderzwaan",
      "owner": ["/organization/nlesc","/organization/uva"],
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/j.vanderzwaan","/person/l.buitinck","/person/p.bos"],
      "user": ["/organization/nlesc"],
      "involvedOrganization": ["/organization/nlesc","/organization/uva"],
      "usedIn": ["/project/dilipad"],
      "startDate": "2016-03-17",
      "status": "inactive",
      "dependency": null,
      "dependencyOf": null,
      "technologyTag": ["Topic Modeling","Latent Dirichlet Allocation","Gibbs Sampler"],
      "title": "Cptm",
      "slug": "cptm",
      
      "description": "<p>A Gibbs sampler that implements Cross-Perspective Topic Modeling, as described in</p>\n\n<blockquote>\n  <p>Fang, Si, Somasundaram, &amp; Yu (2012). Mining Contrastive Opinions on Political Texts using Cross-Perspective Topic Model. In proceedings of the fifth ACM international conference on Web Search and Data Mining. http://dl.acm.org/citation.cfm?id=2124306</p>\n</blockquote>\n\n<p>The cross-perspective topic model is an extended form of Latent Dirichlet Allocation\n(LDA). Topics are learned by doing LDA on the topic words (nouns) in\nthe corpus. Opinions are learned from a separate LDA process using opinion words\n(adjectives, verbs, and adverbs). A topic is a probability distribution\nover topic words. An opinion is a probability distribution over opinion words.\nWhile the topics are shared among the entire corpus, opinions depend on the perspective\na document belongs to. A document can only belong to a single perspective, and the\ndivision of the corpus in perspectives is fixed and must be known in advance.</p>\n\n<p>The imaginary process for generating documents is: one first selects a topic,\nbased on the topic mixture of that document. Then a topic word is drawn from the\ntopic. This procedure is repeated until all topic words have been selected.\nNext, one selects an opinion based on the frequency of topic words associated\nwith the topics in the document. The more words associated with a certain topic,\nthe higher the chance that the corresponding opinion will be selected. The\ncontents of the opinion (i.e., probabilities of opinion words) depend on the\ngenerator’s perspective. Next, an opinion word is drawn from the selected opinion.\nThis procedure is again repeated until all opinion words have been selected.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/datavaults/",
      "id": "/software/datavaults",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "name": "DataVaults",
      "endorsedBy": ["/organization/nlesc"],
      "tagLine": "Technology of Attachment to a DBMS of large file repositories.",
      "codeRepository": "https://github.com/MonetDB/data-vaults",
      "website": "https://www.monetdb.org/Documentation/Extensions/DataVaults",
      "nlescWebsite": null,
      "documentationUrl": "https://www.monetdb.org/Documentation/Extensions/DataVaults",
      "downloadUrl": "https://www.monetdb.org/Downloads",
      "doi": null,
      "logo": null,
      "programmingLanguage": ["C"],
      "license": ["mpl-2.0"],
      "competence": ["Optimized Data Handling"],
      "discipline": ["eScience Methodology"],
      "expertise": ["Databases"],
      "supportLevel": "specialized",
      "contactPerson": "/person/r.goncalves",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/r.goncalves"],
      "user": ["/person/r.goncalves"],
      "involvedOrganization": ["/organization/cwi","/organization/monetdb","/organization/jhu"],
      "usedIn": ["/project/big-data-analytics-in-the-geo-spatial-domain","/project/3d-geospatial-data-exploration-for-modern-risk-management-systems","/project/massive-point-clouds-for-esciences"],
      "startDate": "2014-03-1",
      "status": "active",
      "dependency": null,
      "dependencyOf": ["/software/monetdb"],
      "technologyTag": ["Scientific Databases","In-situ data access"],
      "title": "Datavaults",
      "slug": "datavaults",
      
      "description": "<p>A data vault provides a symbiosis between a DBMS and existing file-based repositories.\nIt keeps data in its original format while scalable processing functionality is offered\nthrough the DBMS. The concept was designed by the CWI Database group, currently the\nNetherlands eScience center works closely with CWI Database group to improve it and\nextend it.</p>\n\n<h1 id=\"why-datavaults\">Why DataVaults?</h1>\n\n<p>DataVaults provides transparent access to all data kept in the file repository\nthrough a tabular or array-based interface abstraction. The in-situ data access\nis possible due to the large amounts of metadata (data of data) existent on file\nformats such as NetCDF, LAS/LAZ, MSEEDS, HDF5, etc. Such metadata is used for\neffective data skipping, but also to collect data insights, e.g. summaries and\nsamples, without having to process the entire data set.</p>\n\n<p>Such an approach gives the user the opportunity to continue performing data\ncuration activities since the main data archive is the file-based repository,\ni.e., the raw data is kept outside of the DBMS.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/differential-evolution/",
      "id": "/software/differential-evolution",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/NLeSC/DifferentialEvolution",
      "competence": ["Big Data Analytics"],
      "contactPerson": "/person/j.spaaks",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/j.spaaks"],
      "dependency": ["JFreeChart","D3"],
      "discipline": ["eScience Methodology"],
      "documentationUrl": null,
      "expertise": ["Information Visualization","Scientific Visualization","Machine Learning"],
      "involvedOrganization": ["/organization/nlesc"],
      "license": ["apache-2.0"],
      "name": "Differential Evolution",
      "endorsedBy": ["/organization/nlesc"],
      "owner": ["/organization/nlesc","/person/j.spaaks"],
      "programmingLanguage": ["Java","JavaScript"],
      "startDate": "2013-06-18",
      "status": "inactive",
      "supportLevel": "specialized",
      "tagLine": "Differential Evolution global optimization algorithm, with Metropolis for uncertainty estimation",
      "technologyTag": ["Visualization","Global Optimization","Uncertainty Estimation","Parameter Estimation","Calibration"],
      "usedIn": null,
      "user": ["/organization/nlesc","/person/j.spaaks"],
      "website": "https://github.com/NLeSC/DifferentialEvolution",
      "title": "Differential Evolution",
      "slug": "differential-evolution",
      
      "description": "<p>Java implementation of the Differential Evolution algorithm by Storn &amp; Price.</p>\n\n<p>Additionally uses Metropolis algorithm to estimate the parameter uncertainty.</p>\n\n<p>The software includes some simple\nvisualizations using JFreeChart (Java) as well as some simple D3.js (JavaScript).</p>\n\n<p>Standard plotting routines include:</p>\n\n<ol>\n  <li>marginal parameter histograms;</li>\n  <li>matrix of 2-D parameter correlations (scatter);</li>\n  <li>matrix of 2-D parameter correlations (heatmap);</li>\n  <li>parameter evolution scatter;</li>\n  <li>objective score evolution scatter.</li>\n</ol>\n\n<p>Currently, the Differential Evolution algorithm can be used to optimize any one of 6 models:</p>\n\n<table>\n  <thead>\n    <tr>\n      <th>index</th>\n      <th>Java model class name</th>\n      <th>number of parameters</th>\n      <th>description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>CubicModel</td>\n      <td>4</td>\n      <td>polynomial</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>DoubleNormalModel</td>\n      <td>1</td>\n      <td>two Gaussians, benchmark check on the accuracy of the Metropolis part</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>LinearDynamicModel</td>\n      <td>1</td>\n      <td>draining linear tank</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>RastriginModel</td>\n      <td>2</td>\n      <td>benchmark model with response surface containing many local minima</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>RosenbrockModel</td>\n      <td>2</td>\n      <td>benchmark model with response surface containing many local minima, large insensitive areas, and curved ridges</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>SingleNormalModel</td>\n      <td>1</td>\n      <td>simpler version of the DoubleNormalModel</td>\n    </tr>\n  </tbody>\n</table>\n\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/docker-couch-admin/",
      "id": "/software/docker-couch-admin",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "name": "Docker Couch Admin",
      "endorsedBy": ["/organization/nlesc"],
      "tagLine": "Configures a web service using angular-schema-form and CouchDB",
      "codeRepository": "https://github.com/NLeSC/docker-couch-admin",
      "downloadUrl": "https://hub.docker.com/r/nlesc/couch-admin/",
      "doi": "http://dx.doi.org/10.5281/zenodo.61301",
      "programmingLanguage": ["JavaScript","HTML","Dockerfile"],
      "license": ["apache-2.0"],
      "competence": ["Efficient Computing"],
      "discipline": ["eScience Methodology"],
      "expertise": ["Information Integration"],
      "supportLevel": "specialized",
      "contactPerson": "/person/j.borgdorff",
      "owner": ["/organization/nlesc"],
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/j.borgdorff"],
      "user": ["/organization/nlesc"],
      "involvedOrganization": ["/organization/nlesc"],
      "usedIn": ["/project/simcity"],
      "startDate": "2016-01-07",
      "status": "inactive",
      "dependency": ["/software/couchdb"],
      "dependencyOf": null,
      "technologyTag": ["Docker","CouchDB","AngularJS","JSON Schema"],
      "title": "Docker Couch Admin",
      "slug": "docker-couch-admin",
      
      "description": "<p>A Docker image with a customisable CouchDB administration console, using\nangular-schema-form. All configuration is readable from the CouchDB database.</p>\n\n<p>This is specifically used to have dynamic but shared configuration in a set of\nDocker containers. The configuration is accessible from a web interface.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/eastroviz/",
      "id": "/software/eastroviz",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "name": "eAstroViz",
      "endorsedBy": ["/organization/nlesc"],
      "tagLine": "This tool can convert and visualize radio astronomy measurement sets, as well as most LOFAR intermediate data producs. It also does RFI mitigation.",
      "codeRepository": "https://github.com/NLeSC/eAstroViz",
      "programmingLanguage": ["Java"],
      "license": ["apache-2.0"],
      "competence": ["Optimized Data Handling"],
      "discipline": ["Physics & Beyond","eScience Methodology"],
      "expertise": ["Handling Sensor Data"],
      "supportLevel": "specialized",
      "contactPerson": "/person/r.vannieuwpoort",
      "owner": ["/organization/nlesc"],
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/r.vannieuwpoort"],
      "user": ["/person/r.vannieuwpoort"],
      "involvedOrganization": ["/organization/nlesc","/organization/astron"],
      "usedIn": ["/project/beyond-the-data-explosion"],
      "startDate": "2013-04-12",
      "status": "active",
      "dependency": null,
      "dependencyOf": null,
      "technologyTag": ["Library"],
      "title": "Eastroviz",
      "slug": "eastroviz",
      
      "description": "<p>Visualization of the Netherlands eScience center eAstronomy project.</p>\n\n<p>This tool can convert and visualize radio astronomy measurement sets,\nas well as most LOFAR intermediate data producs, such as raw voltages,\nfiltered data and beam formed data. In addition, this tool can perform\nRFI mitigation.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/eecology-annotation/",
      "id": "/software/eecology-annotation",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/NLeSC/eEcology-Annotation-WS",
      "competence": ["Optimized Data Handling"],
      "contactPerson": "/person/s.verhoeven",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/s.verhoeven"],
      "dependency": ["/software/extjs-datetime"],
      "discipline": ["Environment & Sustainability"],
      "doi": "http://dx.doi.org/10.5281/zenodo.45200",
      "downloadUrl": "https://github.com/NLeSC/eEcology-Annotation-WS/releases",
      "license": ["apache-2.0"],
      "expertise": ["Handling Sensor Data","Information Retrieval","Scientific Visualization","Databases"],
      "involvedOrganization": ["/organization/nlesc","/organization/uva","/organization/surfsara"],
      "name": "eEcology Annotation Tool",
      "endorsedBy": ["/organization/nlesc"],
      "owner": ["/organization/nlesc","/organization/uva","/person/s.verhoeven"],
      "programmingLanguage": ["Python","JavaScript"],
      "startDate": "2013-03-21",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "Visualize & annotate GPS measurements of bird movements",
      "technologyTag": ["GIS","Website"],
      "usedIn": ["/project/eecology"],
      "user": ["/organization/uva"],
      "website": "https://services.e-ecology.sara.nl/aws/",
      "title": "Eecology Annotation",
      "slug": "eecology-annotation",
      
      "description": "<p>Rich web interface to virtualize &amp; annotate GPS measurements.\nThe visualization has been applied to <a href=\"http://www.uva-bits.nl/system\">uva-bits tracking devices</a> , but can be used for any spatial-temporal data.\nThe annotation tool is a <a href=\"http://uva-bits.nl/virtual-lab\">Virtual lab</a> of the uva-bits project.</p>\n\n<p><img src=\"https://github.com/NLeSC/eEcology-Annotation-UI/raw/master/resources/screenshot.png\" alt=\"Screenshot of annotation application\" title=\"Screenshot\" /></p>\n\n<p>The production web application is hosted at SurfSARA and requires membership of the UvA-BiTS community.\nA demo available <a href=\"http://nlesc.github.io/eEcology-Annotation-UI/demo/demo.html\">here</a> or without annotations <a href=\"http://nlesc.github.io/eEcology-Annotation-UI/demo/demo-na.html\">here</a>.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/eecology-tracker-calendar/",
      "id": "/software/eecology-tracker-calendar",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/NLeSC/eEcology-script-wrapper",
      "competence": ["Optimized Data Handling"],
      "contactPerson": "/person/s.verhoeven",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/s.verhoeven"],
      "discipline": ["Environment & Sustainability"],
      "expertise": ["Handling Sensor Data","Information Retrieval","Scientific Visualization","Databases"],
      "involvedOrganization": ["/organization/nlesc","/organization/uva","/organization/surfsara"],
      "name": "eEcology Tracker calendar",
      "endorsedBy": ["/organization/nlesc"],
      "license": ["apache-2.0"],
      "owner": ["/organization/nlesc","/organization/uva","/person/s.verhoeven"],
      "programmingLanguage": ["Python"],
      "startDate": "2013-02-15",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "Calendar overview with daily statistics of GPS-tracker",
      "technologyTag": ["GIS","Website"],
      "usedIn": ["/project/eecology"],
      "user": ["/organization/uva"],
      "website": "https://public.e-ecology.sara.nl/sw/tool/calendar/",
      "title": "Eecology Tracker Calendar",
      "slug": "eecology-tracker-calendar",
      
      "description": "<p>The tracker calendar shows daily statistics or metrics of tracker as a calendar heatmap. It can be used to find days when something interesting happened or to find repeating patterns.</p>\n\n<p><img src=\"/images/eecology-tracker-calendar.png\" alt=\"Screenshot of tracker calendar\" title=\"Screenshot\" /></p>\n\n<p>The following metrics of a tracker are calculated for each day:</p>\n\n<ul>\n  <li>Nr. of GPS measurements</li>\n  <li>Nr. of accelerometer measurements</li>\n  <li>2D distance travelled (km): Distance of line which consists of GPS measurements from midnight till midnight, altitude is ignored.</li>\n  <li>Maximum altitude (m): Highest absolute altitude</li>\n  <li>Average altitude (m): Average absolute altitude</li>\n  <li>Minimum altitude (m): Lowest absolute altitude</li>\n  <li>Maximum temperature (°C)</li>\n  <li>Average temperature (°C)</li>\n  <li>Minimum temperature (°C)</li>\n  <li>Minimum voltage battery (V): Lowest battery voltage</li>\n  <li>Maximum interval between GPS measurements (hh:mm:ss)</li>\n  <li>Minimum interval between GPS measurements (hh:mm:ss)</li>\n</ul>\n\n<p>The color range can be clipped to find outliers.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/ewaterleaf/",
      "id": "/software/ewaterleaf",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/eWaterCycle/eWaterleaf",
      "contactPerson": "/person/n.drost",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/n.drost"],
      "name": "eWaterLeaf",
      "endorsedBy": ["/organization/nlesc"],
      "tagLine": "Web-based visualization for the eWaterCycle project",
      "license": ["apache-2.0"],
      "discipline": ["Environment & Sustainability"],
      "competence": ["Big Data Analytics"],
      "expertise": ["Scientific Visualization"],
      "programmingLanguage": ["JavaScript"],
      "technologyTag": ["Website"],
      "supportLevel": "basic",
      "status": "inactive",
      "startDate": "2014-06-06",
      "owner": ["/organization/nlesc"],
      "title": "Ewaterleaf",
      "slug": "ewaterleaf",
      
      "description": "<p>eWaterLeaf is a simple web-based visualization for the eWaterCycle project. It relies heavily on the Leaflet Javascript library, and ncWMS Web Map Service implementation.</p>\n\n<p>eWaterLeaf mostly serves as an exploration tool for the data generated by the PCRGLOB-WB model used in the eWaterCycle project, but it should work with any dataset compatible with ncWMS (NetCDF-CF files).</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/extjs-datetime/",
      "id": "/software/extjs-datetime",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "name": "ExtJS-DateTime",
      "endorsedBy": ["/organization/nlesc"],
      "tagLine": "DateTime form input field for ExtJS",
      "website": "https://github.com/NLeSC/ExtJS-DateTime",
      "codeRepository": "https://github.com/NLeSC/ExtJS-DateTime",
      "programmingLanguage": ["JavaScript"],
      "competence": ["Optimized Data Handling"],
      "discipline": ["Environment & Sustainability"],
      "expertise": ["Handling Sensor Data","Information Retrieval","Scientific Visualization","Databases"],
      "license": ["apache-2.0"],
      "supportLevel": "specialized",
      "contactPerson": "/person/s.verhoeven",
      "owner": ["/organization/nlesc","/organization/uva"],
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/s.verhoeven"],
      "involvedOrganization": ["/organization/nlesc","/organization/uva","/organization/surfsara"],
      "usedIn": ["/project/eecology"],
      "user": ["/organization/uva"],
      "startDate": "2013-08-23",
      "status": "active",
      "dependencyOf": ["/software/eecology-annotation"],
      "technologyTag": ["Web UI component","ExtJS"],
      "title": "Extjs Datetime",
      "slug": "extjs-datetime",
      
      "description": "<p>DateTime form input field for <a href=\"https://www.sencha.com/products/extjs/\">ExtJS</a> JavaScript framework.</p>\n\n<p>ExtJS had a date input field and a time input field, this field combines them into a single field.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/fairdatapoint/",
      "id": "/software/fairdatapoint",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "name": "FAIR Data Point",
      "endorsedBy": ["/organization/nlesc"],
      "tagLine": "FAIR Data Point Metadata Service",
      "codeRepository": "https://github.com/NLeSC/ODEX-FAIRDataPoint",
      "nlescWebsite": "https://www.esciencecenter.nl/technology/software/fairdatapoint",
      "documentationUrl": null,
      "logo": null,
      "downloadUrl": "https://github.com/NLeSC/ODEX-FAIRDataPoint",
      "website": "http://fdp.biotools.nl:8080",
      "programmingLanguage": ["Python","Java"],
      "license": ["apache-2.0"],
      "competence": ["Optimized Data Handling"],
      "discipline": ["Life Sciences & eHealth","eScience Methodology"],
      "expertise": ["Linked Data","Information Integration"],
      "supportLevel": "specialized",
      "contactPerson": "/person/a.kuzniar",
      "owner": ["/organization/nlesc"],
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/a.kuzniar",{"name":"Rajaram Kaliyaperuma","githubUrl":"https://github.com/rajaram5","affiliation":["/organization/lumc"]},{"name":"Luiz Olavo Bonino da Silva Santos","linkedInUrl":"http://www.linkedin.com/in/luizolavo","affiliation":["/organization/dtl"]}],
      "user": ["/person/a.kuzniar","/organization/nlesc","/organization/lumc","/organization/wur"],
      "involvedOrganization": ["/organization/nlesc","/organization/lumc","/organization/dtl"],
      "usedIn": ["/project/odex4all","/project/candygene"],
      "startDate": "2015-06-01",
      "status": "active",
      "dependency": null,
      "dependencyOf": null,
      "technologyTag": ["FAIR Data","Linked Data","Semantic Web","RDF","JSON-LD","Turtle","RDF/XML","N-Triples","DCAT","Dublin Core metadata","RESTful API"],
      "title": "Fairdatapoint",
      "slug": "fairdatapoint",
      
      "description": "<p>FAIR Data Point (FDP) is one of the components of the FAIR Data e-infrastructure. FDP enables both i) the data owners to expose data sets in compliance with the <a href=\"http://www.force11.org/group/fairgroup/fairprinciples\">FAIR Data Guiding Principles</a> and ii) the data users to discover more information about available data sets. Specifically, the FDP addresses the first facet of the FAIRness, namely the findability or discoverability of data, by providing metadata at four complementary levels: about FDP itself, data catalog(s), data set(s) including available distribution(s). The FDP software has been developed as a stand-alone Web application in both Python and Java languages.</p>\n\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/google-earth-toolbox-for-matlab/",
      "id": "/software/google-earth-toolbox-for-matlab",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/scottleedavis/googleearthtoolbox/",
      "competence": ["Big Data Analytics"],
      "contactPerson": "/person/j.spaaks",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": [{"name":"Scott Lee Davis","affiliation":["/organization/uva"],"githubUrl":"https://github.com/scottleedavis"},"/person/j.spaaks"],
      "dependency": ["MATLAB"],
      "discipline": ["eScience Methodology"],
      "documentationUrl": "https://github.com/scottleedavis/googleearthtoolbox/tree/master/matlab/html",
      "expertise": ["Information Visualization","Scientific Visualization"],
      "involvedOrganization": ["/organization/uva"],
      "license": ["lgpl-3.0"],
      "name": "GoogleEarth Toolbox for MATLAB",
      "endorsedBy": ["/organization/nlesc"],
      "owner": ["/person/j.spaaks",{"name":"Scott Lee Davis"},"/organization/uva"],
      "programmingLanguage": ["MATLAB"],
      "startDate": "2006-11-10",
      "status": "inactive",
      "supportLevel": "specialized",
      "tagLine": "Export data from MATLAB to GoogleEarth's KML format.",
      "technologyTag": ["Visualization","Maps","GoogleEarth","MATLAB"],
      "usedIn": null,
      "user": ["/person/j.spaaks",{"name":"Scott Lee Davis"}],
      "website": "https://github.com/scottleedavis/googleearthtoolbox/",
      "title": "Google Earth Toolbox For Matlab",
      "slug": "google-earth-toolbox-for-matlab",
      
      "description": "<p>The MATLAB functions within this toolbox let you visualize your spatially distributed data in GoogleEarth by automatically generating KML-formatted files.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/heem-dataset/",
      "id": "/software/heem-dataset",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "name": "Historic Embodied Emotions Model (HEEM) dataset",
      "endorsedBy": ["/organization/nlesc"],
      "tagLine": "279 17th and 18th century Dutch theater texts with HEEM labels",
      "doi": "http://dx.doi.org/10.5281/zenodo.47751",
      "license": ["cc-by"],
      "competence": ["Big Data Analytics"],
      "discipline": ["Humanities & Social Sciences"],
      "expertise": ["Text Mining"],
      "supportLevel": "specialized",
      "contactPerson": "/person/j.vanderzwaan",
      "owner": ["/organization/nlesc","/organization/vua"],
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/j.vanderzwaan"],
      "user": ["/organization/nlesc"],
      "involvedOrganization": ["/organization/nlesc","/organization/vua"],
      "usedIn": ["/project/from-sentiment-mining-to-mining-embodied-emotions","/project/visualizing-uncertainty-and-perspectives"],
      "startDate": "2016-03-17",
      "status": "inactive",
      "technologyTag": ["Dataset","Emotion Mining"],
      "title": "Heem Dataset",
      "slug": "heem-dataset",
      
      "description": "<p>279 17th and 18th century Dutch theater texts in NAF format. 29 texts are manually annotated with\nHEEM labels. Machine learning was applied to predict HEEM labels for all texts.</p>\n\n<p>The dataset also contains metadata about the texts (title, authors, genre, period, etc.).</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/kernel_tuner/",
      "id": "/software/kernel_tuner",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/benvanwerkhoven/kernel_tuner",
      "competence": ["Efficient Computing"],
      "contactPerson": "/person/b.vanwerkhoven",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/b.vanwerkhoven"],
      "discipline": ["eScience Methodology"],
      "documentationUrl": "http://benvanwerkhoven.github.io/kernel_tuner/sphinxdoc/html/index.html",
      "expertise": ["High Performance Computing","Accelerated Computing"],
      "license": ["apache-2.0"],
      "name": "Kernel Tuner",
      "endorsedBy": ["/organization/nlesc"],
      "owner": ["/organization/nlesc","/person/b.vanwerkhoven"],
      "programmingLanguage": ["Python","CUDA","OpenCL"],
      "startDate": "2016-03-27",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "A simple CUDA/OpenCL kernel tuner in Python.",
      "technologyTag": ["GPU Computing"],
      "user": ["/person/b.vanwerkhoven"],
      "usedIn": ["/project/a-jungle-computing-approach-to-large-scale-online-forensic-analysis","/project/3d-geospatial-data-exploration-for-modern-risk-management-systems"],
      "website": "http://benvanwerkhoven.github.io/kernel_tuner/",
      "title": "Kernel_tuner",
      "slug": "kernel_tuner",
      
      "description": "<p>The goal of this project is to provide a - as simple as possible - tool for tuning CUDA and OpenCL kernels.</p>\n\n<h1 id=\"kernel-tuning\">Kernel Tuning</h1>\n\n<p>A very common problem in GPU programming is that some combination of thread block dimensions and other kernel parameters, like tiling or unrolling factors, results in dramatically better performance than other kernel configurations. The goal of auto-tuning is to automate the process of finding the best performing configuration for a given device.</p>\n\n<p>This kernel tuner aims that you can directly use the tuned kernel without introducing any new dependencies. The tuned kernels can afterwards be used independently of the programming environment, whether that is using C/C++/Java/Fortran or Python doesn’t matter.</p>\n\n<p>The kernel_tuner module currently only contains one function which is called tune_kernel to which you pass at least the kernel name, a string containing the kernel code, the problem size, a list of kernel function arguments, and a dictionary of tunable parameters. There are also a lot of optional parameters, for a full list see the full documentation.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/knime-archetype/",
      "id": "/software/knime-archetype",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "name": "Knime node archetype",
      "endorsedBy": ["/organization/nlesc"],
      "tagLine": "Generator for Knime workflow node skeleton repository with sample code.",
      "codeRepository": "https://github.com/3D-e-Chem/tycho-knime-node-archetype",
      "website": "https://github.com/3D-e-Chem/tycho-knime-node-archetype",
      "downloadUrl": "https://bintray.com/nlesc/tycho-knime-node-archetype/tycho-knime-node-archetype/",
      "programmingLanguage": ["Java"],
      "license": ["apache-2.0"],
      "competence": ["Optimized Data Handling"],
      "discipline": ["Life Sciences & eHealth"],
      "expertise": ["Information Integration"],
      "supportLevel": "specialized",
      "contactPerson": "/person/s.verhoeven",
      "owner": ["/organization/nlesc","/organization/radboud.university.nijmegen","/organization/vua"],
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/s.verhoeven"],
      "involvedOrganization": ["/organization/nlesc","/organization/radboud.university.nijmegen","/organization/vua"],
      "usedIn": ["/project/3d-e-chem"],
      "startDate": "2015-10-01",
      "status": "active",
      "technologyTag": ["Knime","Maven","Scaffold"],
      "title": "Knime Archetype",
      "slug": "knime-archetype",
      
      "description": "<p><a href=\"https://travis-ci.org/3D-e-Chem/tycho-knime-node-archetype\"><img src=\"https://travis-ci.org/3D-e-Chem/tycho-knime-node-archetype.svg?branch=master\" alt=\"Build Status\" /></a>\n<a href=\"https://bintray.com/nlesc/tycho-knime-node-archetype/tycho-knime-node-archetype/_latestVersion\"> <img src=\"https://api.bintray.com/packages/nlesc/tycho-knime-node-archetype/tycho-knime-node-archetype/images/download.svg\" alt=\"Download\" /> </a></p>\n\n<p>Generates <a href=\"http://www.knime.org\">Knime</a> workflow node skeleton repository with sample code.</p>\n\n<p>The <a href=\"https://maven.apache.org/guides/introduction/introduction-to-archetypes.html\">Maven archetype</a> will generate a multi-module project.\nThe project can be build resulting in a Eclipse update site.\nNodes can be installed in Knime using the update site.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/liblas/",
      "id": "/software/liblas",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "http://www.liblas.org/",
      "competence": ["Big Data Analytics"],
      "contactPerson": "/person/o.rubi",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/o.rubi","/person/r.goncalves"],
      "discipline": ["eScience Methodology"],
      "expertise": null,
      "involvedOrganization": ["/organization/nlesc"],
      "license": ["bsd-2-clause"],
      "name": "libLAS",
      "programmingLanguage": ["C++","C"],
      "startDate": "2007-01-01",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "C/C++ library for reading and writing the very common LAS Lidar format.",
      "user": ["/organization/nlesc","/person/o.rubi","/person/r.goncalves"],
      "usedIn": ["/project/massive-point-clouds-for-esciences","/project/big-data-analytics-in-the-geo-spatial-domain","/project/3d-geospatial-data-exploration-for-modern-risk-management-systems"],
      "technologyTag": ["Point clouds","Library"],
      "title": "Liblas",
      "slug": "liblas",
      
      "description": "<p>libLAS is a C/C++ library for reading and writing the very common LAS Lidar format and its compressed version (LAZ). The NLeSC contribution to the repository has been (so far) two binary converters from LAS/LAZ to PostgreSQL and MonetDB binary dumpt formats.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/magma/",
      "id": "/software/magma",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/NLeSC/Osmium",
      "competence": ["Big Data Analytics","Efficient Computing"],
      "contactPerson": "/person/l.ridder",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/l.ridder","/person/s.verhoeven","/person/m.sanders"],
      "dependency": ["/software/osmium","/software/rdkit"],
      "discipline": ["Life Sciences & eHealth"],
      "endorsedBy": ["/organization/nlesc"],
      "expertise": ["High Performance Computing","Scientific Visualization"],
      "involvedOrganization": ["/organization/nlesc","/organization/wur"],
      "license": ["apache-2.0"],
      "name": "MAGMa",
      "owner": ["/organization/nlesc","/organization/wur"],
      "programmingLanguage": ["Python","JavaScript"],
      "startDate": "2011-05-10",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "MAGMa is an online application for the automatic chemical annotation of accurate multistage MSn spectral data.",
      "technologyTag": ["Distributed","Webservice"],
      "usedIn": ["/project/emetabolomics"],
      "user": ["/person/l.ridder","/person/m.sanders"],
      "website": "http://www.emetabolomics.org/magma",
      "title": "Magma",
      "slug": "magma",
      
      "description": "<p>MAGMa is an online application for the automatic chemical annotation of accurate multistage MSn spectral data.</p>\n\n<ul>\n  <li>MSn data can be uploaded as a hierarchical tree of fragment peaks, either based on m/z values or elemental formulas, or as an mzXML file of the raw data.</li>\n  <li>Candidate molecules are automatically retrieved from PubChem, from a subset of PubChem compounds present in Kegg, or from the Human Metabolome Database.</li>\n  <li>Candidate molecules can be predicted based on in silico reaction rules describing microbiotic and human biotransformations</li>\n  <li>For each candidate molecule, substructures are generated and matched with the observed fragment peaks.</li>\n  <li>The web browser enables efficient mining of the automatically annotated data.</li>\n  <li>Open Source, source code available at <a href=\"https://github.com/NLeSC/MAGMa\">https://github.com/NLeSC/MAGMa</a></li>\n</ul>\n\n<p><img src=\"https://github.com/NLeSC/MAGMa/raw/master/web/magmaweb/static/img/metabolites.png\" alt=\"screenshot\" title=\"Screenshot of web application\" />.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/magnesium/",
      "id": "/software/magnesium",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/NLeSC/eSalsa-MPI",
      "competence": ["Efficient Computing"],
      "contactPerson": "/person/j.maassen",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/j.maassen"],
      "dependencyOf": ["/project/esalsa"],
      "discipline": ["eScience Methodology"],
      "expertise": ["Distributed Computing"],
      "involvedOrganization": ["/organization/nlesc"],
      "license": ["apache-2.0"],
      "name": "Magnesium",
      "endorsedBy": ["/organization/nlesc"],
      "owner": ["/organization/nlesc","/person/j.maassen"],
      "programmingLanguage": ["C","Java"],
      "startDate": "2012-05-1",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "An MPI wrapper library that allows traditional MPI applications to be run on a combination of multiple supercomputers or clusters, without changing a single line of code in the application.",
      "technologyTag": ["Distributed","Library"],
      "usedIn": ["/project/esalsa"],
      "user": ["/organization/nlesc","/person/j.maassen"],
      "title": "Magnesium",
      "slug": "magnesium",
      
      "description": "<p>Magnesium is an MPI wrapper library that allows traditional MPI \napplications to be run on a combination of multiple supercomputers \nor clusters, without changing a single line of code in the application.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/massivepotreeconverter/",
      "id": "/software/massivepotreeconverter",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/NLeSC/Massive-PotreeConverter",
      "competence": ["Optimized Data Handling"],
      "contactPerson": "/person/o.rubi",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/o.rubi","/person/m.vanmeersbergen","/person/s.verhoeven"],
      "discipline": ["eScience Methodology"],
      "expertise": ["Databases","Distributed Computing"],
      "involvedOrganization": ["/organization/nlesc","/organization/potree"],
      "license": ["bsd-2-clause"],
      "name": "Massive-PotreeConverter",
      "endorsedBy": ["/organization/nlesc"],
      "programmingLanguage": ["Python"],
      "startDate": "2011-01-01",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "Distributed generation of massive multi-resolution octrees (required by Potree-based renderers)",
      "user": ["/organization/nlesc","/person/o.rubi"],
      "usedIn": ["/project/massive-point-clouds-for-esciences"],
      "owner": ["/organization/nlesc"],
      "dependencyOf": ["/software/ahn2webviewer"],
      "dependency": ["/software/potreeconverter","/software/potree"],
      "technologyTag": ["Point clouds"],
      "title": "Massivepotreeconverter",
      "slug": "massivepotreeconverter",
      
      "description": "<p>This repository extends the PotreeConverter (&lt;/software/potreeconverter&gt;) through a collection of Python scripts to make it able to convert massive point clouds to the potree format (octree).</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/matrix-of-scatter/",
      "id": "/software/matrix-of-scatter",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/jspaaks/matrix-of-scatter",
      "competence": ["Big Data Analytics"],
      "contactPerson": "/person/j.spaaks",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/j.spaaks"],
      "dependency": ["https://plot.ly/python/","https://www.python.org/"],
      "discipline": ["eScience Methodology"],
      "documentationUrl": "http://jspaaks.github.io/matrix-of-scatter/docs/",
      "expertise": ["Information Visualization","Scientific Visualization"],
      "involvedOrganization": ["/organization/nlesc"],
      "license": ["apache-2.0"],
      "name": "Matrix of scatter",
      "endorsedBy": ["/organization/nlesc"],
      "owner": ["/person/j.spaaks"],
      "programmingLanguage": ["Python3"],
      "startDate": "2016-03-16",
      "status": "wip",
      "supportLevel": "specialized",
      "tagLine": "Visualization of multi-dimensional data using a matrix of linked scatter plots.",
      "technologyTag": ["Visualization","Python","matrix of scatter","trellis","n-dimensional","multi-dimensional","Plotly"],
      "usedIn": ["/project/sherlock"],
      "user": ["/person/j.spaaks"],
      "website": "https://github.com/jspaaks/matrix-of-scatter",
      "title": "Matrix Of Scatter",
      "slug": "matrix-of-scatter",
      
      "description": "<p>Visualization of multi-dimensional data using a matrix of linked scatter plots</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/metrochart/",
      "id": "/software/metrochart",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/jspaaks/metrochart",
      "competence": ["Big Data Analytics"],
      "contactPerson": "/person/j.spaaks",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/j.spaaks"],
      "discipline": ["eScience Methodology"],
      "documentationUrl": "http://jspaaks.github.io/metrochart/tsdoc",
      "expertise": ["Information Visualization","Scientific Visualization"],
      "involvedOrganization": ["/organization/nlesc"],
      "license": ["apache-2.0"],
      "name": "metrochart.js",
      "endorsedBy": ["/organization/nlesc"],
      "owner": ["/organization/nlesc"],
      "programmingLanguage": ["TypeScript"],
      "startDate": "2015-11-17",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "Graph visualization using metrolines.",
      "technologyTag": ["Visualization","Website","Graph","Timeline"],
      "usedIn": ["/project/sherlock"],
      "user": ["/organization/nlesc","/organization/nfi","/person/j.spaaks"],
      "website": "http://jspaaks.github.io/metrochart",
      "title": "Metrochart",
      "slug": "metrochart",
      
      "description": "<p>TypeScript library to visualize graphs in the style of metroline maps.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/micmac/",
      "id": "/software/micmac",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://geoportail.forge.ign.fr/hg/culture3d",
      "competence": ["Optimized Data Handling"],
      "contactPerson": "/person/o.rubi",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/o.rubi"],
      "discipline": ["eScience Methodology"],
      "expertise": ["Handling Sensor Data","High Performance Computing"],
      "involvedOrganization": ["/organization/nlesc","/organization/ign","/organization/utwente"],
      "license": ["CeCILL-B"],
      "name": "MicMac",
      "endorsedBy": ["/organization/nlesc"],
      "programmingLanguage": ["C++"],
      "startDate": "2013-10-01",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "Open Source photogrammetry toolset (from images to point clouds)",
      "user": ["/organization/nlesc","/person/o.rubi"],
      "usedIn": ["/project/improving-photogrammetry"],
      "owner": ["/organization/ign"],
      "technologyTag": ["Point clouds","Photogrammetry"],
      "title": "Micmac",
      "slug": "micmac",
      
      "description": "<p>MicMac is an open source set of tools that executes the various steps of the photogrammetric workflow (from images to a point cloud).</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/mmsoda-toolbox-for-matlab/",
      "id": "/software/mmsoda-toolbox-for-matlab",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/NLeSC/esibayes",
      "competence": ["Big Data Analytics","Efficient Computing"],
      "contactPerson": "/person/j.spaaks",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/j.spaaks",{"name":"Willem Bouten","affiliation":["/organization/uva"]},{"name":"Jasper A. Vrugt","affiliation":["/organization/uva"]},{"name":"Jeroen Engelberts","affiliation":["/organization/surfsara"]}],
      "dependencyOf": null,
      "discipline": ["eScience Methodology"],
      "documentationUrl": "https://github.com/NLeSC/esibayes/blob/master/manual/out/2013-mmsoda-manual.pdf",
      "downloadUrl": "https://github.com/NLeSC/esibayes/archive/master.zip",
      "expertise": ["Data Assimilation","Distributed Computing","High Performance Computing"],
      "involvedOrganization": ["/organization/uva"],
      "license": ["apache-2.0"],
      "logo": null,
      "name": "MMSODA Toolbox for MATLAB",
      "endorsedBy": ["/organization/nlesc"],
      "nlescWebsite": "https://www.esciencecenter.nl/project/esibayes",
      "owner": ["/organization/nlesc","/person/j.spaaks","/organization/uva"],
      "programmingLanguage": ["MATLAB","C"],
      "startDate": "2012-12-18",
      "status": "inactive",
      "supportLevel": "specialized",
      "tagLine": "MATLAB toolbox for global optimization and state estimation of dynamic models using distributed computing",
      "technologyTag": ["Cluster Computing","Global Optimization","Inverse Modeling","SCEM-UA","SODA","Single-objective","Multi-objective","Dynamic Modeling","Uncertainty Estimation"],
      "usedIn": ["/project/esibayes"],
      "user": ["/person/j.spaaks","/organization/nlesc"],
      "website": "https://github.com/NLeSC/esibayes",
      "title": "Mmsoda Toolbox For Matlab",
      "slug": "mmsoda-toolbox-for-matlab",
      
      "description": "<p>An eScience infrastructure for Bayesian inverse modeling</p>\n\n<p>We have developed a parallel MATLAB version of SODA (Vrugt et al., 2005) that makes use of MPI. The software package is called ‘The MMSODA Toolbox for MATLAB’, or MMSODA for short (MMSODA stands for MATLAB-MPI-SODA).</p>\n\n<p>MMSODA offers the functionality of a number of previously separate softwares, namely SCEM-UA (Vrugt <em>et al</em>., 2003b), SODA (Vrugt <em>et al.</em>, 2005a,b; Clark and Vrugt, 2006), MOSCEM-UA (Vrugt <em>et al.</em>, 2003a), multi-objective SODA (Vrugt <em>et al.</em>, 2008), the MPITB-parallel version of SCEM-UA implemented in Octave (Vrugt <em>et al.</em>, 2006b), and the MPITB-parallel version of SODA implemented in Octave (Vrugt <em>et al.</em>, 2006a). Additionally, MMSODA offers a parallel version of multi-objective SCEM-UA, and a parallel version of multi-objective SODA, both of which did not exist previously. Moreover, MMSODA does not use Octave when running in parallel, because Octave does not evaluate code as quickly as does MATLAB. MMSODA circumvents (in a legal fashion) the license requirements that are often an impediment to parallel computation by compiling the MATLAB code into a binary which can be run without any license. Compiling the binary, however, does require a license, both for the MATLAB program itself, as well as for the MATLAB Compiler Runtime Toolbox. Fortunately, the required licenses are available on most cluster environments targeting a scientific and engineering audience.</p>\n\n<p>In short, the acronyms mentioned above mean that: MMSODA can do parameter tuning with or without intermediate state updating by an ensemble Kalman Filter; that MMSODA supports both single-objective and multi-objective optimization; and that the optimization can be run either sequentially on a local machine, or in parallel on a cluster computer.</p>\n\n<p>The serial/parallel capability is particularly attractive, since it allows the users to set up their optimizations locally on their own machines, thus ensuring a familiar development environment without the need to make the code compatible with Octave syntax. When the user finishes setting up the optimization, running it on a cluster computer is simply a matter of copying the relevant directory to the cluster storage using standard tools (e.g. WinSCP) and compiling the software by executing a script that comes with the software. Furthermore, MMSODA is fully documented with HTML documentation which can be accessed in the same way as MATLAB’s built-in commands, namely through the <code class=\"highlighter-rouge\">doc</code> command.</p>\n\n<p><em>References</em></p>\n\n<ul>\n  <li>J. A. Vrugt, H. V. Gupta, L. A. Bastidas, W. Bouten, and S. Sorooshian. Effective and efficient algorithm for multi-objective optimization of hydrologic models. Water Resources Research, 39(8):1214, 2003a. doi: 10.1029/2002WR001746.</li>\n  <li>J. A. Vrugt, H. V. Gupta, W. Bouten, and S. Sorooshian. A Shuffled Complex Evolution Metropolis algorithm for optimization and uncertainty assessment of hydrologic model parameters. Water Resources Research, 39(8):1201, 2003b. doi: 10.1029/2002WR001642.</li>\n  <li>Jasper A. Vrugt, Cees G. H. Diks, Hoshin V. Gupta, Willem Bouten, and Jacobus M. Verstraten. Improved treatment of uncertainty in hydrologic modeling: Combining the strengths of global optimization and data assimilation. Water Resources Research, 41:W01017, 2005a. doi: 10.1029/2004WR003059.</li>\n  <li>Jasper A. Vrugt, Bruce A. Robinson, and Velimir V. Vesselinov. Improved inverse modeling for flow and transport in subsurface media: Combined parameter and state estimation. Geophysical Research Letters, 32:L18408, 2005b. doi: 10.1029/2005GL023940.</li>\n  <li>Jasper A. Vrugt, Hoshin V. Gupta, Breandánn Ó Nualláin, and Willem Bouten. Real-time data assimilation for operational ensemble streamflow forecasting. Journal of Hydrometeorology, 7(3):548–575, 2006a. doi: 10.1175/JHM504.1.</li>\n  <li>Jasper A. Vrugt, Breandánn Ó Nualláin, Bruce A. Robinson, Willem Bouten, Stefan C. Dekker, and Peter M. A. Sloot. Application of parallel computing to stochastic parameter estimation in environmental models. Computers &amp; Geosciences, 32:1139–1155, 2006b. doi:10.1016/j.cageo.2005.10.015.</li>\n  <li>Jasper A. Vrugt, Philip H. Stauffer, Th. Wöhling, Bruce A. Robinson, and Velimir V. Vesselinov. Inverse modeling of subsurface flow and transport properties: A review with new developments. Vadose Zone Journal, 7(2):843–864, 2008. doi: 10.2136/vzj2007.0078.</li>\n</ul>\n\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/monetdb/",
      "id": "/software/monetdb",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://www.monetdb.org/Downloads",
      "competence": ["Optimized Data Handling"],
      "contactPerson": "/person/r.goncalves",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/r.goncalves","/person/v.hees"],
      "dependency": ["/software/datavaults"],
      "discipline": ["eScience Methodology"],
      "documentationUrl": "https://www.monetdb.org/Documentation",
      "downloadUrl": "https://www.monetdb.org/Downloads",
      "expertise": ["Databases"],
      "endorsedBy": ["/organization/nlesc"],
      "involvedOrganization": ["/organization/cwi","/organization/monetdb"],
      "license": ["mpl-2.0"],
      "logo": "/images/software/monetdb.png",
      "name": "MonetDB",
      "programmingLanguage": ["C"],
      "startDate": "2014-03-01",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "A relational column-oriented Database Management System.",
      "technologyTag": ["Column-stores","Relational Databases","DBMS","Scientific Databases"],
      "usedIn": ["/project/big-data-analytics-in-the-geo-spatial-domain","/project/3d-geospatial-data-exploration-for-modern-risk-management-systems","/project/massive-point-clouds-for-esciences","/project/compressing-the-sky-into-a-large-collection-of-statistical-models"],
      "user": ["/person/r.goncalves","/person/o.rubi","/person/v.hees"],
      "website": "https://en.wikipedia.org/wiki/MonetDB",
      "title": "Monetdb",
      "slug": "monetdb",
      
      "description": "<p>MonetDB is an open source column-oriented database management system developed\nat the Centrum Wiskunde &amp; Informatica (CWI) in the Netherlands. It was designed\nto provide high performance on complex queries against large databases, such as\ncombining tables with hundreds of columns and multi-million rows. MonetDB has\nbeen applied in high-performance applications for online analytical processing\n(OLAP), data mining, GIS, text retrieval, sequence alignment processing and in\nearth observation applications.</p>\n\n<h1 id=\"why-monetdb\">Why MonetDB?</h1>\n\n<p>MonetDB is DBMS which provides storage model based on vertical fragmentation,\na modern CPU-tuned query execution architecture, automatic and self-tuning indexes,\nrun-time query optimization, and a modular software architecture. It has been\nused by the Netherlands eScience Center for efficient spatio-temporal analysis.</p>\n\n<p>With in-house expertise on MonetDB architecture, the Netherlands eScience center\ncontributes to the design, research and implementation of new functionality to\nsupport geo-spatial applications. Architectural improvements and new Geo-spatial\nfunctionality, following OGS standards, were added to provide efficiency and\nscalability to applications running on top of PostGIS or other commercial Spatial\nDBMSs.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/netcdf2littler/",
      "id": "/software/netcdf2littler",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "name": "NetCDF2LittleR",
      "endorsedBy": ["/organization/nlesc"],
      "tagLine": "A fortran application to convert NetCDF files to the Little-R format.",
      "codeRepository": "https://github.com/rvanharen/netcdf2littler",
      "downloadUrl": "https://github.com/rvanharen/netcdf2littler/releases",
      "programmingLanguage": ["FORTRAN"],
      "license": ["apache-2.0","bsd-3-clause (custom extension)"],
      "competence": ["Big Data Analytics"],
      "discipline": ["Environment & Sustainability","eScience Methodology"],
      "expertise": ["Data Assimilation"],
      "supportLevel": "specialized",
      "contactPerson": "/person/r.vanharen",
      "owner": ["/person/r.vanharen"],
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/r.vanharen"],
      "user": ["/person/r.vanharen","/organization/wur"],
      "involvedOrganization": ["/organization/nlesc"],
      "usedIn": ["/project/era-urban"],
      "startDate": "2015-6-17",
      "status": "wip",
      "dependency": ["NetCDF","udunits2"],
      "dependencyOf": null,
      "technologyTag": ["FileConversion"],
      "title": "Netcdf2littler",
      "slug": "netcdf2littler",
      
      "description": "<p>A FORTRAN application to convert NetCDF files to the Little-R format. The Little-R format is the accepted input format for the WRF data assimilation system (WRFDA) preprocessor (obsproc).</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/noodles/",
      "id": "/software/noodles",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/NLeSC/noodles",
      "competence": ["Efficient Computing"],
      "contactPerson": "/person/j.hidding",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/j.hidding","/person/b.weel",{"affiliation":["/organization/vua"],"githubUrl":"https://github.com/felipeZ","name":"Felipe Zapata"}],
      "dependency": ["/software/xenon","/software/pyxenon"],
      "dependencyOf": ["/pymicmac","/software/pymicmac"],
      "discipline": ["Life Sciences & eHealth","eScience Methodology"],
      "documentationUrl": "http://nlesc.github.io/noodles/sphinxdoc/html/index.html",
      "expertise": ["Orchestrated Computing"],
      "endorsedBy": ["/organization/nlesc"],
      "involvedOrganization": ["/organization/nlesc","/organization/vua"],
      "license": ["lgpl-3.0"],
      "name": "Noodles",
      "nlescWebsite": "https://www.esciencecenter.nl/technology/software/noodles",
      "owner": ["/organization/nlesc"],
      "programmingLanguage": ["Python"],
      "startDate": "2015-10-11",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "Programmable workflow engine for Python.",
      "technologyTag": ["Workflow"],
      "usedIn": ["/project/computational-chemistry-made-easy","/project/improving-photogrammetry"],
      "user": ["/organization/nlesc","/organization/vua","/person/j.hidding","/person/o.rubi","/person/l.ridder"],
      "website": "http://nlesc.github.io/noodles/",
      "title": "Noodles",
      "slug": "noodles",
      
      "description": "<p>Noodles is a programmable workflow engine for Python. It can be used to parallelize your code with minimal effort.</p>\n\n<h1 id=\"why-noodles\">Why Noodles?</h1>\n\n<p>The primary goal of Noodles is to make it easy to run jobs on cluster supercomputers, in parallel, straight from a Python shell. The user enters a Python script that looks and feels like a serial program. The Noodles engine then converts this script into a call graph. This graph can be executed on a variety of machines using the different back-end runners that Noodles provides. This is not so much a design driven by technology but by social considerations. The end user may expect an elegant, easy to understand, interface to a computational library. This user experience we refer to as eating of noodles.</p>\n\n<p>The computational library that is exposed to the user by means of Noodles needs to adhere to some design principles that are more strict than plain Python gives us. The library should follow a functional style of programming and is limited by the fact that function arguments need to pass through a layer where data is converted to and from a JSON format. The design of such a library is the cooking of noodles. As it is with ramen noodles, ofttimes the cook is also an avid consumer of noodles.</p>\n\n<p>The complexity of running a workflow in parallel on a wide variety of architectures is taken care of by the Noodles engine. This is the production of noodles which is left as an exercise for the Noodles dev-team at the Netherlands eScience Center.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/openda/",
      "id": "/software/openda",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/n.drost"],
      "name": "OpenDA",
      "endorsedBy": ["/organization/nlesc"],
      "tagLine": "Data Assimilation Toolbox",
      "usedIn": ["/project/ewatercycle","/project/large-scale-data-assimilation"],
      "contactPerson": "/person/n.drost",
      "user": ["/person/n.drost"],
      "discipline": ["Environment & Sustainability","eScience Methodology"],
      "competence": ["Optimized Data Handling"],
      "expertise": ["Data Assimilation"],
      "programmingLanguage": ["Java"],
      "license": ["lgpl-3"],
      "supportLevel": "basic",
      "owner": [{"name":"OpenDA Association"}],
      "status": "active",
      "involvedOrganization": ["/organization/nlesc","/organization/tu-delft","/organization/deltares"],
      "startDate": "2010-01-01",
      "technologyTag": ["Calibration"],
      "title": "Openda",
      "slug": "openda",
      
      "description": "<p>OpenDA is an open interface standard for (and free implementation of) a set of tools to quickly implement data-assimilation and calibration for arbitrary numerical models. OpenDA wants to stimulate the use of data-assimilation and calibration by lowering the implementation costs and enhancing the exchange of software among researchers and end-users.\nA model that conforms to the OpenDA standard can use all the tools that are available in OpenDA. This allows experimentation with data-assimilation/calibration methods without the need for extensive programming. Reversely, developers of data-assimilation/calibration software that make their implementations compatible with the OpenDA interface will make their new methods usable for all OpenDA users (either for free or on a commercial basis).\nOpenDA has been designed for high performance. Hence, even large-scale models can use it. Also, OpenDA allows users to optimize the interaction between their model and the data-assimilation/calibration methods. Hence, data-assimilation with OpenDA can be as efficient as with custom-made implementations of data-assimilation methods.\nOpenDA is an Open Source project. Contributions are welcome from anyone wishing to participate in the further development of the OpenDA toolset.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/osmium/",
      "id": "/software/osmium",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/NLeSC/osmium",
      "competence": ["Optimized Data Handling"],
      "contactPerson": "/person/s.verhoeven",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/s.verhoeven","/person/j.borgdorff"],
      "dependency": ["/software/xenon"],
      "dependencyOf": ["/software/magma"],
      "discipline": ["Physics & Beyond","eScience Methodology"],
      "downloadUrl": "https://github.com/NLeSC/osmium/releases",
      "expertise": ["Distributed Computing"],
      "endorsedBy": ["/organization/nlesc"],
      "involvedOrganization": ["/organization/nlesc","/organization/wur"],
      "license": ["apache-2.0"],
      "logo": "/images/software/osmium.png",
      "name": "Osmium",
      "nlescWebsite": "https://www.esciencecenter.nl/technology/software/osmium",
      "owner": ["/organization/nlesc","/organization/wur","/person/s.verhoeven"],
      "programmingLanguage": ["Java"],
      "startDate": "2013-04-12",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "Web service to submit jobs via a Xenon supported scheduler.",
      "technologyTag": ["Distributed","Webservice"],
      "usedIn": ["/project/emetabolomics"],
      "user": ["/person/s.verhoeven","/person/l.ridder"],
      "website": "https://github.com/NLeSC/osmium",
      "title": "Osmium",
      "slug": "osmium",
      
      "description": "<p>At the center we developed Xenon library which can be used to submit calculations to schedulers or batch queue systems on high performance computing infrastructure like clusters. Xenon requires Java programming knowledge. To make the Xenon schedulers available in other languages than Java a web service was made called Osmium. With Osmium calculations can be submitted using any programming language to any of the Xenon supported schedulers.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/pattyanalytics/",
      "id": "/software/pattyanalytics",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/NLeSC/PattyAnalytics",
      "competence": ["Big Data Analytics"],
      "contactPerson": "/person/j.attema",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/j.attema","/person/l.buitinck","/person/j.borgdorff","/person/c.martinez"],
      "discipline": ["eScience Methodology"],
      "expertise": ["Databases","Handling Sensor Data"],
      "involvedOrganization": ["/organization/nlesc"],
      "license": ["apache-2.0"],
      "name": "PattyAnalytics",
      "endorsedBy": ["/organization/nlesc"],
      "programmingLanguage": ["Python"],
      "startDate": "2013-10-01",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "Reusable point cloud analytics software (segmentation, registration, file format conversion)",
      "user": ["/organization/nlesc","/person/o.rubi"],
      "usedIn": ["/project/viaappia-patty"],
      "owner": ["/organization/nlesc"],
      "dependency": ["/software/python-pcl"],
      "technologyTag": ["Point clouds","GIS","PCL"],
      "title": "Pattyanalytics",
      "slug": "pattyanalytics",
      
      "description": "<p>Reusable point cloud analytics software. Includes segmentation, registration, file format conversion. This makes use of the python bindings of the Point Cloud Library (PCL; <a href=\"https://github.com/NLeSC/python-pcl\">https://github.com/NLeSC/python-pcl</a>).</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/pattydata/",
      "id": "/software/pattydata",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/NLeSC/PattyData",
      "competence": ["Optimized Data Handling"],
      "contactPerson": "/person/o.rubi",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/o.rubi","/person/e.ranguelova","/person/r.goncalves","/person/r.vanharen"],
      "discipline": ["eScience Methodology"],
      "expertise": ["Databases","Handling Sensor Data"],
      "involvedOrganization": ["/organization/nlesc"],
      "license": ["apache-2.0"],
      "name": "PattyData",
      "endorsedBy": ["/organization/nlesc"],
      "programmingLanguage": ["Python"],
      "startDate": "2013-10-01",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "Data Management scripts for the Via Appia 3D GIS",
      "user": ["/organization/nlesc","/person/o.rubi"],
      "usedIn": ["/project/viaappia-patty"],
      "owner": ["/organization/nlesc"],
      "technologyTag": ["Point clouds","GIS","Dataset"],
      "title": "Pattydata",
      "slug": "pattydata",
      
      "description": "<p>This repository is related to the data management for the Via Appia 3d GIS.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/pattyvis/",
      "id": "/software/pattyvis",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/NLeSC/pattyvis",
      "competence": ["Big Data Analytics"],
      "contactPerson": "/person/m.vanmeersbergen",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/p.bos","/person/o.rubi","/person/m.vanmeersbergen","/person/s.verhoeven","/person/m.kuzak","/person/j.vanderzwaan","/person/b.vanwerkhoven","/person/l.kulik"],
      "discipline": ["eScience Methodology"],
      "expertise": ["Scientific Visualization"],
      "involvedOrganization": ["/organization/nlesc","/organization/vua"],
      "license": ["apache-2.0"],
      "name": "PattyVis",
      "endorsedBy": ["/organization/nlesc"],
      "programmingLanguage": ["JavaScript"],
      "startDate": "2013-01-01",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "WebGL point cloud visualization of the Via Appia based on potree",
      "technologyTag": ["Point clouds","WebGL","GIS"],
      "dependency": ["/software/potree","/software/potreeconverter"],
      "user": ["/organization/nlesc","/person/o.rubi"],
      "usedIn": ["/project/viaappia-patty"],
      "owner": ["/organization/nlesc"],
      "title": "Pattyvis",
      "slug": "pattyvis",
      
      "description": "<p><a href=\"https://travis-ci.org/NLeSC/PattyVis\"><img src=\"https://travis-ci.org/NLeSC/PattyVis.svg?branch=master\" alt=\"Build Status\" /></a>\n<a href=\"https://codeclimate.com/github/NLeSC/PattyVis\"><img src=\"https://codeclimate.com/github/NLeSC/PattyVis/badges/gpa.svg\" alt=\"Code Climate\" /></a>\n<a href=\"https://codeclimate.com/github/NLeSC/PattyVis\"><img src=\"https://codeclimate.com/github/NLeSC/PattyVis/badges/coverage.svg\" alt=\"Test Coverage\" /></a>\n<a href=\"https://saucelabs.com/u/patty-vis\"><img src=\"https://saucelabs.com/buildstatus/patty-vis\" alt=\"Sauce Test Status\" /></a>\n<a href=\"https://david-dm.org/NLeSC/PattyVis#info=devDependencies\"><img src=\"https://david-dm.org/NLeSC/PattyVis/dev-status.svg\" alt=\"devDependency Status\" /></a>\n<a href=\"https://www.codacy.com/public/sverhoeven/PattyVis\"><img src=\"https://www.codacy.com/project/badge/a2ebd9977fe04aa1af6e5c47dc8d6927\" alt=\"Codacy Badge\" /></a>\n<a href=\"http://dx.doi.org/10.5281/zenodo.45923\"><img src=\"https://zenodo.org/badge/doi/10.5281/zenodo.45923.svg\" alt=\"DOI\" /></a></p>\n\n<h2 id=\"webgl-point-cloud-visualization-of-the-via-appia-based-on-httppotreeorg\">Webgl point cloud visualization of the Via Appia based on http://potree.org</h2>\n<p><img src=\"https://github.com/NLeSC/PattyVis/raw/master/DOCS/pattyvis_fp_ss4.png\" alt=\"logo\" title=\"A beautiful vista\" />\nA big step towards a 3D GIS Application.<br />\n<img src=\"https://github.com/NLeSC/PattyVis/raw/master/DOCS/pattyvis_fp_ss2.png\" alt=\"logo\" title=\"A big step towards a 3D GIS Application\" />\nWith 3D footprints of grave monuments based on GPS coordinates.<br />\n<img src=\"https://github.com/NLeSC/PattyVis/raw/master/DOCS/pattyvis_fp_ss1.png\" alt=\"logo\" title=\"With 3D footprints based on GPS coordinates\" />\nA ‘background’ or reference frame was made with Fugro’s drive-map technology http://www.drive-map.eu/<br />\n<img src=\"https://github.com/NLeSC/PattyVis/raw/master/DOCS/pattyvis_fp_ss9.png\" alt=\"logo\" title=\"The drive map visualized\" />\nSeveral monuments have been photographed extensively and made into seperate point clouds. This is an ongoing process.<br />\n<img src=\"https://github.com/NLeSC/PattyVis/raw/master/DOCS/pattyvis_fp_ss5.png\" alt=\"logo\" title=\"Here you can see the drive-map and the site-specific photography based point cloud next to eachother\" />\n<img src=\"https://github.com/NLeSC/PattyVis/raw/master/DOCS/pattyvis_fp_ss3.png\" alt=\"logo\" title=\"A particularly well-captured monument.\" />\nMeasurements can be made in the 3D environment.<br />\n<img src=\"https://github.com/NLeSC/PattyVis/raw/master/DOCS/pattyvis_fp_ss8.png\" alt=\"logo\" title=\"Measurements can be made in the 3D environment.\" />\nHistorical maps can give extra information on the site’s history.<br />\n<img src=\"https://github.com/NLeSC/PattyVis/raw/master/DOCS/pattyvis_fp_ss6.png\" alt=\"logo\" title=\"Historical maps can give extra information on the site's history.\" />\nSearching options like the material used in the site can give extra insight.<br />\n<img src=\"https://github.com/NLeSC/PattyVis/raw/master/DOCS/pattyvis_fp_ss7.png\" alt=\"logo\" title=\"Historical maps can give extra information on the site's history.\" /></p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/pdal/",
      "id": "/software/pdal",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "http://www.pdal.io/",
      "competence": ["Big Data Analytics"],
      "contactPerson": "/person/o.rubi",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/o.rubi"],
      "discipline": ["eScience Methodology"],
      "expertise": null,
      "involvedOrganization": ["/organization/nlesc"],
      "license": ["bsd-2-clause"],
      "name": "PDAL",
      "endorsedBy": ["/organization/nlesc"],
      "programmingLanguage": ["C++"],
      "startDate": "2011-01-01",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "C++ Library for translating and manipulating point cloud data.",
      "user": ["/organization/nlesc","/person/o.rubi"],
      "usedIn": ["/project/massive-point-clouds-for-esciences"],
      "technologyTag": ["Point clouds","Library"],
      "title": "Pdal",
      "slug": "pdal",
      
      "description": "<p>PDAL is a C++ BSD library for translating and manipulating point cloud data. It is very much like the GDAL library which handles raster and vector data.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/picas/",
      "id": "/software/picas",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/sara-nl/picasclient",
      "competence": ["Efficient Computing"],
      "contactPerson": "/person/j.borgdorff",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/j.borgdorff",{"affiliation":["/organization/surfsara"],"name":"Jan Bot"},{"affiliation":["/organization/surfsara"],"name":"Maarten Kooyman"},{"affiliation":["/organization/surfsara"],"name":"Anatoli Danezi"}],
      "dependency": ["/software/couchdb"],
      "dependencyOf": ["/software/couchdb"],
      "discipline": ["eScience Methodology"],
      "expertise": ["Distributed Computing"],
      "involvedOrganization": ["/organization/nlesc","/organization/surfsara"],
      "license": ["mit"],
      "name": "picas",
      "owner": ["/organization/surfsara"],
      "programmingLanguage": ["Python"],
      "startDate": "2012-06-12",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "Simple distributed task execution framework using a CouchDB database",
      "technologyTag": ["Distributed","Library","CouchDB"],
      "usedIn": ["/project/simcity"],
      "user": ["/person/j.borgdorff"],
      "title": "Picas",
      "slug": "picas",
      
      "description": "<p>PiCaS is a simple task execution framework using a CouchDB database. All tasks\nare stored in a single database, before, during, and after processing, thereby\nkeeping a log of all actions.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/pidilib/",
      "id": "/software/pidilib",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "name": "PIDIlib",
      "endorsedBy": ["/organization/nlesc"],
      "tagLine": "A collection of scripts used in the PIDIMEHS project",
      "codeRepository": "https://bitbucket.org/egpbos/pidilib",
      "nlescWebsite": null,
      "website": null,
      "documentationUrl": null,
      "logo": null,
      "programmingLanguage": ["Python"],
      "license": ["apache-2.0"],
      "competence": ["Big Data Analytics"],
      "discipline": ["Humanities & Social Sciences"],
      "expertise": ["Text Mining","Information Retrieval","Information Visualization","Data Assimilation"],
      "supportLevel": "advanced",
      "contactPerson": "/person/p.bos",
      "owner": ["/organization/nlesc"],
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/p.bos","/person/l.buitinck"],
      "user": ["/organization/university.of.groningen"],
      "involvedOrganization": ["/organization/nlesc","/organization/university.of.groningen","/organization/uva","/organization/surfsara"],
      "usedIn": ["/project/pidimehs"],
      "startDate": "2014-08-01",
      "status": "inactive",
      "dependency": null,
      "dependencyOf": null,
      "technologyTag": ["Elasticsearch"],
      "title": "Pidilib",
      "slug": "pidilib",
      
      "description": "<p>PIDIlib is a collection of scripts used in the PIDIMEHS project. It is mainly provided to showcase the possibilities of combining Elasticsearch, Pandas and Matplotlib in Python in the study of history of politics and media. PIDIlib’s main contents are:</p>\n\n<ul>\n  <li>Ways of querying the PIDIMEHS KB newspaper Elasticsearch instance</li>\n  <li>Using Pandas to analyze the queried aggregations</li>\n  <li>Visualization using Matplotlib</li>\n  <li>Data used for ‘indicators of pillarization’ in the PIDIMEHS project</li>\n  <li>Jupyter notebooks that were used to produce the results presented in the PIDIMEHS technical paper.</li>\n</ul>\n\n<p>The KB newspaper data can be freely obtained from <a href=\"www.delpher.nl\">Delpher</a>. It is not allowed to redistribute this data, so we cannot give access to our own instance. To use PIDIlib, one will have to set up their own Elasticsearch instance. Alternatively, one may contact the KB or SURFsara to request access to the SURFsara instance (which we used as well).</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/potree/",
      "id": "/software/potree",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/potree/potree",
      "competence": ["Big Data Analytics"],
      "contactPerson": "/person/m.vanmeersbergen",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/m.vanmeersbergen","/person/o.rubi","/person/s.verhoeven"],
      "dependencyOf": ["/software/ahn2webviewer","/software/massivepotreeconverter","/software/pattyvis"],
      "discipline": ["eScience Methodology"],
      "expertise": ["Scientific Visualization"],
      "involvedOrganization": ["/organization/nlesc","/organization/potree"],
      "license": ["bsd-2-clause"],
      "name": "Potree",
      "endorsedBy": ["/organization/nlesc"],
      "owner": ["/organization/potree"],
      "programmingLanguage": ["JavaScript"],
      "startDate": "2011-01-01",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "WebGL point cloud viewer",
      "user": ["/organization/nlesc","/person/o.rubi"],
      "usedIn": ["/project/massive-point-clouds-for-esciences","/project/viaappia-patty"],
      "technologyTag": ["Point clouds","WebGL","Website"],
      "title": "Potree",
      "slug": "potree",
      
      "description": "<p>WebGL point cloud viewer for large datasets (<a href=\"http://potree.org\">http://potree.org</a>)</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/potreeconverter/",
      "id": "/software/potreeconverter",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/potree/PotreeConverter",
      "competence": ["Optimized Data Handling"],
      "contactPerson": "/person/o.rubi",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/o.rubi"],
      "discipline": ["eScience Methodology"],
      "expertise": ["Scientific Visualization"],
      "involvedOrganization": ["/organization/nlesc","/organization/potree"],
      "license": ["bsd-2-clause"],
      "name": "PotreeConverter",
      "endorsedBy": ["/organization/nlesc"],
      "programmingLanguage": ["C++"],
      "startDate": "2011-01-01",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "Generation of multi-resolution octrees (required by Potree-based renderers)",
      "user": ["/organization/nlesc","/person/o.rubi"],
      "usedIn": ["/project/massive-point-clouds-for-esciences","/project/viaappia-patty"],
      "owner": ["/organization/potree"],
      "dependencyOf": ["/software/ahn2webviewer","/software/massivepotreeconverter","/software/pattyvis"],
      "technologyTag": ["Point clouds","Library"],
      "title": "Potreeconverter",
      "slug": "potreeconverter",
      
      "description": "<p>Builds a potree octree from las, laz, binary ply, xyz or ptx files. This is required by any Potree-based renderer. For massive point clouds use the Massive-PotreeConverter (&lt;/software/massivepotreeconverter&gt;)</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/pycoeman/",
      "id": "/software/pycoeman",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/NLeSC/pycoeman",
      "competence": ["Efficient Computing"],
      "contactPerson": "/person/o.rubi",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/o.rubi"],
      "dependency": null,
      "dependencyOf": ["/software/pymicmac"],
      "discipline": ["eScience Methodology"],
      "downloadUrl": "https://github.com/NLeSC/pycoeman",
      "expertise": ["Distributed Computing"],
      "involvedOrganization": ["/organization/nlesc"],
      "license": ["apache-2.0"],
      "name": "pycoeman",
      "owner": ["/organization/nlesc"],
      "programmingLanguage": ["Python"],
      "startDate": "2016-07-25",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "pycoeman (Python Commands Execution Manager) is a Python toolkit for executing command-line commands.",
      "technologyTag": ["Distributed","Workflow","Library"],
      "usedIn": ["/project/improving-photogrammetry"],
      "user": ["/person/o.rubi"],
      "title": "Pycoeman",
      "slug": "pycoeman",
      
      "description": "<p>Python Commands Execution Manager</p>\n\n<p>pycoeman is a Python toolkit for executing command-line commands. It allows the execution of:</p>\n\n<ul>\n  <li>\n    <p>Sequential commands: this is a chain of command-line commands which will be executed one after the other. In other words, this is a set of commands that you would traditionally execute in a Bash script. Normally there are IO dependencies between commands (one command requires the output from one or previous ones).</p>\n  </li>\n  <li>\n    <p>Parallel commands: this is a set of command-line commands which are executed in parallel. In other words, this is a set of commands that you would traditionally execute in a Bash script with all the commands as background jobs (with the &amp; at the end). There cannot be IO dependencies between commands. This is can be useful for pleasingly parallel solutions, i.e. tools which are single-core at programming level but can be parallelized at data level and usually require some final merging process.</p>\n  </li>\n</ul>\n\n<p>pycoeman adds CPU/MEM/disk monitoring during the execution of the commands and it allows to create clean execution environments for easier management of your executions (the commands will be executed in different folders separated from where the input data is). pycoeman has tools to run both sequential and parallel commands locally (in the local computer), and also to run parallel commands in a set of remote hosts accessible via SSH as well as in SGE clusters (computer clusters with Sun Grid Engine batch-queuing system). pycoeman is configured using XML files.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/pymicmac/",
      "id": "/software/pymicmac",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/ImproPhoto/pymicmac",
      "competence": ["Optimized Data Handling"],
      "contactPerson": "/person/o.rubi",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/o.rubi"],
      "dependency": ["/software/pycoeman","/software/noodles"],
      "dependencyOf": null,
      "discipline": ["eScience Methodology"],
      "downloadUrl": "https://github.com/ImproPhoto/pymicmac",
      "expertise": ["Distributed Computing"],
      "involvedOrganization": ["/organization/nlesc"],
      "license": ["apache-2.0"],
      "name": "pymicmac",
      "owner": ["/organization/nlesc"],
      "programmingLanguage": ["Python"],
      "startDate": "2016-05-03",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "pymicmac provides a python interface for MicMac workflows execution and distributed computing tools for MicMac.",
      "technologyTag": ["Distributed","Workflow","Library","Point clouds"],
      "usedIn": ["/project/improving-photogrammetry"],
      "user": ["/person/o.rubi"],
      "title": "Pymicmac",
      "slug": "pymicmac",
      
      "description": "<p>pymicmac provides a python interface for MicMac workflows execution and distributed computing tools for MicMac. pymicmac uses pycoeman (Python Commands Execution Manager) (https://github.com/NLeSC/pycoeman) which also provides CPU/MEM/disk monitoring.</p>\n\n<p>MicMac is a photogrammetric suite which contains many different tools to execute photogrammetric workflows. In short, a photogrammetric workflow contains at least:</p>\n\n<p>(1) tie-point detection: extraction of key features in images and cross-match between different images to detect tie-points (points in the images that represent the same physical locations).</p>\n\n<p>(2) Estimation of camera positions and orientations and of calibration parameters: mainly the bundle adjustment but may include some preparation and/or refinement steps.</p>\n\n<p>(3) Dense-matching point cloud generation. 3D projection of image pixels to produce the dense point cloud.</p>\n\n<p>pymicmac provides the tool micmac-run-workflow to run photogrammetric workflows with a sequence of MicMac commands. The tool uses the sequential commands execution tool of pycoeman which is configured with a XML configuration file that defines a chain of MicMac commands to be executed sequentially.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/python-pcl/",
      "id": "/software/python-pcl",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/NLeSC/python-pcl",
      "competence": ["Big Data Analytics"],
      "contactPerson": "/person/j.attema",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/j.attema","/person/l.buitinck","/person/j.borgdorff","/person/c.martinez"],
      "discipline": ["eScience Methodology"],
      "expertise": ["Handling Sensor Data"],
      "involvedOrganization": ["/organization/nlesc"],
      "license": ["apache-2.0"],
      "name": "python-pcl",
      "endorsedBy": ["/organization/nlesc"],
      "programmingLanguage": ["Python"],
      "startDate": "2014-10-01",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "Python bindings to Point Cloud Library (PCL)",
      "user": ["/organization/nlesc","/person/o.rubi"],
      "usedIn": ["/project/viaappia-patty"],
      "owner": ["/organization/nlesc"],
      "dependencyOf": ["/software/pattyanalytics"],
      "technologyTag": ["Point clouds"],
      "title": "Python Pcl",
      "slug": "python-pcl",
      
      "description": "<p>This is a small python binding to the pointcloud library. Currently, the following parts of the API are wrapped (all methods operate on PointXYZRGB) point types</p>\n\n<ul>\n  <li>I/O and integration; saving and loading PCD files</li>\n  <li>segmentation</li>\n  <li>SAC</li>\n  <li>smoothing</li>\n  <li>filtering</li>\n</ul>\n\n<p>The code tries to follow the Point Cloud API, and also provides helper function for interacting with NumPy.</p>\n\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/pyxenon/",
      "id": "/software/pyxenon",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/NLeSC/pyxenon",
      "competence": ["Efficient Computing"],
      "contactPerson": "/person/j.borgdorff",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/j.borgdorff","/person/j.hidding","/person/s.verhoeven"],
      "dependency": ["/software/xenon"],
      "dependencyOf": ["/software/noodles","/software/wrfpy"],
      "discipline": ["Physics & Beyond","eScience Methodology"],
      "doi": "http://dx.doi.org/10.5281/zenodo.60929",
      "downloadUrl": "https://pypi.python.org/pypi/pyxenon",
      "expertise": ["Distributed Computing"],
      "involvedOrganization": ["/organization/nlesc"],
      "license": ["apache-2.0"],
      "name": "pyxenon",
      "endorsedBy": ["/organization/nlesc"],
      "owner": ["/organization/nlesc"],
      "programmingLanguage": ["Python"],
      "startDate": "2015-11-29",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "Python wrapper for the Xenon programming interface to various compute and storage resources.",
      "technologyTag": ["Distributed","Library"],
      "usedIn": ["/project/simcity","/project/era-urban"],
      "user": ["/person/j.borgdorff","/person/j.hidding","/person/r.vanharen"],
      "title": "Pyxenon",
      "slug": "pyxenon",
      
      "description": "<p>Xenon is a middleware abstraction library. It provides a simple\nprogramming interface to various pieces of software that can be used to\naccess distributed compute and storage resources. pyxenon is the Python\nwrapper to Xenon.</p>\n\n<h1 id=\"why-xenon\">Why Xenon?</h1>\n\n<p>Xenon is developed by the Netherlands eScience Center as a support\nlibrary for our projects. Several projects develop end-user applications\nthat require access to distributed compute and storage resources. Xenon\nprovides a simple API to access those resources, allowing those\napplications to be developed more rapidly. The experience gained during\nthe development of these end-user applications is used to improve the\nXenon API and implementation.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/rdkit/",
      "id": "/software/rdkit",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "programmingLanguage": ["Python","C++"],
      "license": ["bsd-3-clause"],
      "competence": ["Optimized Data Handling"],
      "discipline": ["Life Sciences & eHealth"],
      "expertise": ["Data Assimilation"],
      "endorsedBy": ["/organization/nlesc"],
      "usedIn": ["/project/emetabolomics","/project/3d-e-chem"],
      "dependencyOf": ["/software/magma"],
      "technologyTag": ["Library","Chemistry"],
      "name": "RDKit",
      "tagLine": "RDKit is a collection of cheminformatics and machine-learning software written in C++ and Python.",
      "codeRepository": "https://github.com/rdkit/rdkit",
      "website": "http://rdkit.org/",
      "documentationUrl": "http://rdkit.org/docs/index.html",
      "downloadUrl": "http://sourceforge.net/projects/rdkit/files",
      "logo": "http://rdkit.org/Images/logo.png",
      "status": "active",
      "title": "Rdkit",
      "slug": "rdkit",
      
      "description": "<p>RDKit is a collection of cheminformatics and machine-learning software written in C++ and Python.</p>\n\n<p>It is used in eMetabolomics project to perform reactions.</p>\n\n<p>It is used in 3D-e-Chem project to visualize molecules in KNIME and to store molecules in SQLite.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/recipy/",
      "id": "/software/recipy",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "name": "ReciPy",
      "endorsedBy": ["/organization/nlesc"],
      "tagLine": "Effortless provenance in Python",
      "codeRepository": "https://github.com/recipy/recipy",
      "programmingLanguage": ["Python"],
      "license": ["apache-2.0"],
      "competence": ["Big Data Analytics"],
      "discipline": ["eScience Methodology"],
      "expertise": ["Reproducible Research"],
      "supportLevel": "advanced",
      "contactPerson": "/person/j.vanderzwaan",
      "owner": [{"name":"Robin Wilson","affiliation":["/organization/uva"],"website":"http://www.rtwilson.com"},"/organization/nlesc"],
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": [{"name":"Robin Wilson","affiliation":["/organization/uva"],"website":"http://www.rtwilson.com"},"/person/j.vanderzwaan"],
      "user": ["/organization/nlesc"],
      "involvedOrganization": ["/organization/nlesc"],
      "startDate": "2015-03-27",
      "status": "active",
      "technologyTag": ["Provenance"],
      "title": "Recipy",
      "slug": "recipy",
      
      "description": "<p>Imagine the situation: You’ve written some wonderful Python code which produces a beautiful graph as an output. You save that graph, naturally enough, as “graph.png”. You run the code a couple of times, each time making minor modifications. You come back to it the next week/month/year. Do you know how you created that graph? What input data? What version of your code? If you’re anything like me then the answer will often, frustratingly, be “no”. Of course, you then waste lots of time trying to work out how you created it, or even give up and never use it in that journal paper that will win you a Nobel Prize…</p>\n\n<p>ReciPy (from <em>recipe</em> and <em>python</em>) is a Python module that will save you from this situation! (Although it can’t guarantee that your resulting paper will win a Nobel Prize!) With the addition of a single line of code to the top of your Python files, ReciPy will log each run of your code to a database, keeping track of the input files, output files and the version of your code, and then let you query this database to find out how you actually did create “graph.png”.</p>\n\n<p>When you import recipy it adds a number of classes to “sys.meta_path”. These are then used by Python as part of the importing procedure for modules. The classes that we add are classes derived from “PatchImporter”, often using the easier interface provided by “PatchSimple”, which allow us to wrap functions that do input/output in a function that calls recipy first to log the information.</p>\n\n<p>Currently, ReciPy provides patches for:</p>\n\n<ul>\n  <li>numpy</li>\n  <li>pandas</li>\n  <li>matplotlib.pyplot</li>\n  <li>gdal</li>\n  <li>sklearn</li>\n  <li>nibabel</li>\n</ul>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/rig/",
      "id": "/software/rig",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/nlesc-sherlock/Rig",
      "competence": ["Big Data Analytics"],
      "contactPerson": "/person/j.vanderzwaan",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/j.vanderzwaan","/person/w.vanhage","/person/o.rubi","/person/l.buitinck"],
      "discipline": ["eScience Methodology"],
      "expertise": ["Text Mining","Information Visualization","Databases","Distributed Computing"],
      "endorsedBy": ["/organization/nlesc"],
      "involvedOrganization": ["/organization/nlesc"],
      "license": ["apache-2.0"],
      "logo": "/images/software/rig.png",
      "name": "Rig",
      "owner": ["/organization/nlesc"],
      "programmingLanguage": ["Python","JavaScript"],
      "startDate": "2016-02-16",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "Big data cleaning toolkit",
      "technologyTag": ["Spark"],
      "usedIn": ["/project/sherlock"],
      "user": ["/organization/nlesc"],
      "title": "Rig",
      "slug": "rig",
      
      "description": "<p>Every data analysis project starts with exploring and cleaning data. For tabular data, there are some tools available that facilitate data pre-processing, such as OpenRefine and Trifacta Wrangler. However, these tools one big disadvantage; they don’t scale to really big data sets. Also, Trifacta Wrangler is not open source. For full text data, no tools for data cleanup exist.</p>\n\n<p>The goal of the data cleaning toolkit subproject is to create a data cleaning toolkit that is open source, supports both tabular data and full text pre-processing, and scales to big data sets. The frontend is a web interface that supports loading the data and creating workflows. Based on user input, it generates scripts that are send to the backend for execution. The backend relies on spark to ensure scalability.</p>\n\n<p>Because building a complete toolkit is is rather ambitious, we focus on the use case of vocabulary cleanup for full text data. When working with full text data, pre-processing often consists of tokenizing texts (i.e., splitting them into words), lematizing or stemming the tokens, and performing some kind of filtering to remove typo’s and other unwanted tokens. The data cleaning toolkit should support customizing these tasks, and provide visualizations that facilitate vocabulary cleanup.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/root-conda-recipes/",
      "id": "/software/root-conda-recipes",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "name": "ROOT-conda-recipes",
      "endorsedBy": ["/organization/nlesc"],
      "tagLine": "Conda recipes for building CERN ROOT binaries and its dependencies, with Python 3 support. It provides a \"pythonic\" interface (pandas DataFrames) to the ROOT I/O format.",
      "codeRepository": "http://github.com/NLeSC/root-conda-recipes",
      "nlescWebsite": "http://www.esciencecenter.nl/technology/software/root-conda-recipes",
      "website": "http://www.gitbook.com/book/nlesc/cern-root-conda-recipes/details",
      "documentationUrl": "http://www.gitbook.com/book/nlesc/cern-root-conda-recipes/details",
      "downloadUrl": "http://github.com/NLeSC/root-conda-recipes/archive/master.zip",
      "doi": "http://dx.doi.org/10.5281/zenodo.47512",
      "programmingLanguage": ["Python"],
      "license": ["apache-2.0"],
      "competence": ["Efficient Computing"],
      "discipline": ["Physics & Beyond"],
      "expertise": ["Distributed Computing"],
      "supportLevel": "specialized",
      "contactPerson": "/person/d.remenska",
      "owner": ["/person/d.remenska"],
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/d.remenska","/person/s.verhoeven"],
      "user": ["/organization/nikhef"],
      "involvedOrganization": ["/organization/nikhef"],
      "usedIn": ["/project/pandas-root"],
      "status": "active",
      "dependency": ["ROOT","root-numpy","rootpy"],
      "dependencyOf": null,
      "technologyTag": ["Anaconda","pandas DataFrame","Computing model"],
      "badges": ["[![Build Status](https://api.travis-ci.org/NLeSC/root-conda-recipes.svg)](https://travis-ci.org/NLeSC/root-conda-recipes/)","[![DOI](https://zenodo.org/badge/20885/NLeSC/root-conda-recipes.svg)](https://zenodo.org/badge/latestdoi/20885/NLeSC/root-conda-recipes)","[![Join the chat at https://gitter.im/NLeSC/root-conda-recipes](https://badges.gitter.im/NLeSC/root-conda-recipes.svg)](https://gitter.im/NLeSC/root-conda-recipes?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)"],
      "title": "Root Conda Recipes",
      "slug": "root-conda-recipes",
      
      "description": "<p>This repository contains Conda recipes for building CERN <a href=\"https://root.cern.ch/\">ROOT</a> binaries and its dependencies. For the needs of the <a href=\"http://www.xenon1t.org/\">XENON Dark Matter project</a>, the goal is to provide a “pythonic” interface to the ROOT I/O format, primarily for loading and saving Pandas dataframes in the ROOT format. For this purpose, there are also recipes for building conda binaries of <a href=\"https://github.com/rootpy/root_numpy\">root-numpy</a> and <a href=\"https://github.com/rootpy/rootpy\">rootpy</a>, the community-driven initiative to provide a more pythonic interface with ROOT on top of the existing PyROOT bindings.</p>\n\n<p>The most most important thing for making things work out of the box is the ABI (binary) compatibility between different gcc(libstdc++)/glibc library versions, on various linux distributions. Typically ROOT would even complain when the GCC headers are not of the same version as the one used for building it, so <em>shipping the full GCC and glibc as a run dependency</em> of ROOT, seemed like the best solution.</p>\n\n<p>Combine this with the fact that ROOT 6 requires GCC&gt;=4.8, while we want things to work on older platforms with no “sudo” required, <strong>we decided to fix our GCC distribution to (a relatively recent one) 4.8.2, built against a rather old glibc version 2.12</strong>, making it as cross platform as possible.</p>\n\n<p>Working ROOT has been tested on: Ubuntu 11.10, 12.04, 14.04, 15.04, SLC-6.7, SLC-7. Please try it out and let us know if you experience problems.</p>\n\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/salient-region-detectors/",
      "id": "/software/salient-region-detectors",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "name": "Salient Region Detectors",
      "endorsedBy": ["/organization/nlesc"],
      "tagLine": "Software package for detecting salient regions in images.",
      "codeRepository": "https://github.com/NLeSC/SalientDetector-python",
      "website": null,
      "documentationUrl": "http://nlesc.github.io/SalientDetector-python",
      "logo": null,
      "programmingLanguage": ["Python","MATLAB"],
      "license": ["apache-2.0"],
      "competence": ["Big Data Analytics"],
      "discipline": ["eScience Methodology"],
      "expertise": ["Computer Vision"],
      "supportLevel": "specialized",
      "contactPerson": "/person/e.ranguelova",
      "owner": ["/organization/nlesc"],
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/e.ranguelova","/person/d.vankuppevelt"],
      "user": ["/organization/nlesc"],
      "involvedOrganization": ["/organization/nlesc"],
      "usedIn": null,
      "startDate": "2015-04-01",
      "status": "wip",
      "dependency": null,
      "dependencyOf": null,
      "technologyTag": ["OpenCV"],
      "title": "Salient Region Detectors",
      "slug": "salient-region-detectors",
      
      "description": "<p>This package provide functionality to detect salient regions in images. Salient regions, or features, are regions in the image that are ‘interesting’, such as corners, lines and blobs. The detectors in this package specifically find blob-like regions. The detected regions could be used, for example, to match photos of the same object, taken under different conditions. These salient regions are invariant under transformations such as blurring, light change and rotation.</p>\n\n<p>The software is implemented both in MATLAB and Python.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/sfm/",
      "id": "/software/sfm",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/NLeSC/structure-from-motion",
      "competence": ["Efficient Computing"],
      "contactPerson": "/person/n.drost",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/n.drost","/person/j.spaaks","/person/j.maassen"],
      "discipline": ["eScience Methodology"],
      "expertise": ["Handling Sensor Data","High Performance Computing"],
      "involvedOrganization": ["/organization/nlesc"],
      "license": ["apache-2.0"],
      "name": "Structure-From-Motion",
      "endorsedBy": ["/organization/nlesc"],
      "programmingLanguage": ["Python","C++"],
      "startDate": "2013-10-01",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "Structure from Motion pipeline (from images to point clouds)",
      "user": ["/organization/nlesc","/person/o.rubi"],
      "usedIn": ["/project/viaappia-patty"],
      "owner": ["/organization/nlesc"],
      "technologyTag": ["Point clouds","Photogrammetry"],
      "title": "Sfm",
      "slug": "sfm",
      
      "description": "<p>This repo contains a complete Structure from Motion pipeline. Structure from Motion is a technique to construct a 3d point cloud from a set of images (or a video) of an object. The software in this repository relies heavily on a number of third party libaries, notably Bundler, CMVS, PMVS, and SIFT.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/storyteller/",
      "id": "/software/storyteller",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "name": "StoryTeller",
      "endorsedBy": ["/organization/nlesc"],
      "tagLine": "Visualizing complex humanities data",
      "codeRepository": "https://github.com/NLeSC/UncertaintyVisualization",
      "nlescWebsite": null,
      "website": "http://nlesc.github.io/UncertaintyVisualization/",
      "documentationUrl": null,
      "logo": null,
      "programmingLanguage": ["Python","JavaScript"],
      "license": ["apache-2.0"],
      "competence": ["Big Data Analytics"],
      "discipline": ["Humanities & Social Sciences"],
      "expertise": ["Information Visualization","Text Mining"],
      "supportLevel": "specialized",
      "contactPerson": "/person/m.vanmeersbergen",
      "owner": ["/organization/nlesc"],
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/m.vanmeersbergen","/person/j.vanderzwaan"],
      "user": ["/organization/nlesc"],
      "involvedOrganization": ["/organization/nlesc"],
      "usedIn": ["/project/visualizing-uncertainty-and-perspectives"],
      "startDate": "2014-12-18",
      "status": "active",
      "dependency": null,
      "dependencyOf": null,
      "technologyTag": ["Data-Driven Documents (D3.js)","Crossfilter","dc.js"],
      "title": "Storyteller",
      "slug": "storyteller",
      
      "description": "<p>Storyteller, a visualization tool that helps to analyze complex, multilayered data. This is accomplished by allowing the user to interactively explore the data by adjusting the queries.\nIn addition, Storyteller takes provenance into account by allowing the user to view the data in the original context of their source.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/texcavator/",
      "id": "/software/texcavator",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "name": "Texcavator",
      "endorsedBy": ["/organization/nlesc"],
      "tagLine": "Texcavator is a text mining application used for historical research",
      "codeRepository": "https://github.com/UUDigitalHumanitieslab/texcavator",
      "nlescWebsite": null,
      "website": "http://texcavator.surfsaralabs.nl/",
      "documentationUrl": null,
      "logo": null,
      "programmingLanguage": ["Python","JavaScript"],
      "license": ["apache-2.0"],
      "competence": ["Big Data Analytics"],
      "discipline": ["Humanities & Social Sciences"],
      "expertise": ["Information Retrieval","Text Mining","Distributed Computing","Information Visualization"],
      "supportLevel": "advanced",
      "contactPerson": "/person/j.vanderzwaan",
      "owner": ["/organization/nlesc","/organization/uva","/organization/uu"],
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/j.vanderzwaan",{"name":"Martijn van der Klis","affiliation":["/organization/uu"],"website":"http://www.uu.nl/staff/MHvanderKlis/"}],
      "user": ["/organization/uu"],
      "involvedOrganization": ["/organization/nlesc","/organization/uva","/organization/uu"],
      "usedIn": ["/project/texcavator","/project/shico"],
      "startDate": "2013-12-04",
      "status": "inactive",
      "dependency": null,
      "dependencyOf": null,
      "technologyTag": ["Elasticsearch","Django","Dojo Toolkit"],
      "title": "Texcavator",
      "slug": "texcavator",
      
      "description": "<p>Texcavator is a text mining application used for historical research using newspaper data (the KB newspaper archive).\nThe newspaper articles are stored in an Elasticsearch index.\nDjango is used for user management and other functionality that requires storing information in a relational database (as opposed to storing documents in Elasticsearch). Because the KB data is not public, every user has their own login. Also, users can save queries (currently, a saved query is required to be able to generate word clouds and time lines). User data is stored in a (MySQL) database; Django provides a convenient interface to the database and provides standard functionality for manipulating database records. A disadvantage of Django is that the front end (user interface) and back end are somewhat intertwined. On top of that, the user interface (implemented in Dojo version 1.8.6) directly retrieved and updated information in the database in some cases.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/topic-coherence-for-dutch/",
      "id": "/software/topic-coherence-for-dutch",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "name": "Palmetto position storing Lucene index of Dutch Wikipedia",
      "endorsedBy": ["/organization/nlesc"],
      "tagLine": "Dutch language resource for calculating topic coherence with Palmetto",
      "codeRepository": null,
      "nlescWebsite": null,
      "website": null,
      "documentationUrl": null,
      "logo": null,
      "doi": "http://dx.doi.org/10.5281/zenodo.46377",
      "programmingLanguage": null,
      "license": ["cc-by-sa"],
      "competence": ["Big Data Analytics"],
      "discipline": ["Humanities & Social Sciences"],
      "expertise": ["Text Mining"],
      "supportLevel": "specialized",
      "contactPerson": "/person/j.vanderzwaan",
      "owner": ["/organization/nlesc","/organization/uva"],
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/j.vanderzwaan"],
      "user": ["/organization/nlesc"],
      "involvedOrganization": ["/organization/nlesc","/organization/uva"],
      "usedIn": ["/project/dilipad"],
      "startDate": "2016-02-22",
      "status": "inactive",
      "dependency": null,
      "dependencyOf": null,
      "technologyTag": ["Dataset","Topic Modeling","Topic Coherence","Palmetto"],
      "title": "Topic Coherence For Dutch",
      "slug": "topic-coherence-for-dutch",
      
      "description": "<p>Dutch language resource for calculating topic coherence with <a href=\"http://aksw.org/Projects/Palmetto.html\">Palmetto</a> [1]. The dataset is a position storing Lucene index of the <a href=\"https://dumps.wikimedia.org/nlwiki/20151102/\">Dutch Wikipedia</a>. It was created in the context of the <a href=\"https://www.esciencecenter.nl/project/dilipad\">Netherlands eScience Center Dilipad project</a>. The pdf file contains the results of a case study that shows best topic coherence measure for topics consisting of Dutch nouns is NPMI.</p>\n\n<p>More details can be found in the README.</p>\n\n<p>[1] M. Roeder, A. Both, and A. Hinneburg. Exploring the space of topic coherence measures. In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, pages 399–408, 2015.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/twiqs.nl/",
      "id": "/software/twiqs.nl",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "competence": ["Big Data Analytics"],
      "contactPerson": "/person/e.tjongkimsang",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/e.tjongkimsang"],
      "discipline": ["Humanities & Social Sciences"],
      "expertise": ["Text Mining","Information Retrieval"],
      "involvedOrganization": ["/organization/nlesc","/organization/radboud.university.nijmegen","/organization/surfsara"],
      "name": "twiqs.nl",
      "endorsedBy": ["/organization/nlesc"],
      "owner": ["/organization/nlesc","/organization/radboud.university.nijmegen","/organization/surfsara","/person/e.tjongkimsang"],
      "programmingLanguage": ["Perl","Java"],
      "license": ["apache-2.0"],
      "startDate": "2013-02-22",
      "status": "inactive",
      "supportLevel": "basic",
      "tagLine": "Searchable Dutch tweets",
      "technologyTag": ["Hadoop","Twitter","Website"],
      "usedIn": ["/project/twinl"],
      "website": "http://twiqs.nl/",
      "title": "Twiqs.nl",
      "slug": "twiqs.nl",
      
      "description": "<p>twiqs.nl provides researchers and students the opportunity to search through Dutch tweets. You can look up words and find out where, when and how often they are used, by whom and with what other words they frequently occur together.</p>\n\n<p>The service is based on an existing system set up at the ISLA (UvA) and the RUG with infrastructure from SURFsara - mapping these tweets is a very compute intensive activity. The Twitter API, providing free access to approximately 1% of all tweets worldwide, is constantly harvested and the resulting data stored. Interfaces to this data provide users with a number of analysis tools that can be run on all content and metadata.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/wrfpy/",
      "id": "/software/wrfpy",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "name": "WRFpy",
      "endorsedBy": ["/organization/nlesc"],
      "tagLine": "Python workflow application to set up/run WRF simulations (optionally including data assimilation).",
      "codeRepository": "https://github.com/rvanharen/wrfpy",
      "downloadUrl": "https://github.com/rvanharen/wrfpy/releases",
      "programmingLanguage": ["Python"],
      "license": ["apache-2.0"],
      "competence": ["Big Data Analytics","Efficient Computing"],
      "discipline": ["Environment & Sustainability","eScience Methodology"],
      "expertise": ["Data Assimilation","Distributed Computing","High Performance Computing","Databases"],
      "supportLevel": "specialized",
      "contactPerson": "/person/r.vanharen",
      "owner": ["/person/r.vanharen"],
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/r.vanharen"],
      "user": ["/person/r.vanharen","/organization/wur"],
      "involvedOrganization": ["/organization/nlesc"],
      "usedIn": ["/project/era-urban"],
      "startDate": "2015-10-15",
      "status": "wip",
      "dependency": ["/software/pyxenon","f90nml","WRF","WRFDA","UPP","WPS"],
      "dependencyOf": null,
      "technologyTag": ["Simulation","Workflow"],
      "title": "Wrfpy",
      "slug": "wrfpy",
      
      "description": "<p>WRFpy is a python application that provides an easy way to set up, run,\nand monitor (long) Weather Research and Forecasting (WRF) simulations. It \nprovides a simple user-editable JSON configuration file and an integration\nwith pyxenon to access distributes computing and storage resources. \nOptionally, WRFpy allows for data assimilation using WRF data assimilation\nsystem (WRFDA) and postprocessing of wrfinput files using the NCEP Unified \nPost Processing System (UPP).</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/xenon/",
      "id": "/software/xenon",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/NLeSC/Xenon",
      "competence": ["Efficient Computing"],
      "contactPerson": "/person/j.maassen",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/j.maassen","/person/s.verhoeven","/person/n.drost","/person/r.vannieuwpoort","/person/j.borgdorff","/person/c.meijer","/person/b.vanwerkhoven"],
      "dependencyOf": ["/software/noodles","/software/pyxenon","/software/osmium"],
      "discipline": ["Physics & Beyond","eScience Methodology"],
      "documentationUrl": "http://nlesc.github.io/Xenon/versions/1.1.0/javadoc",
      "doi": "http://dx.doi.org/10.5281/zenodo.35415",
      "downloadUrl": "https://bintray.com/nlesc/xenon/xenon/view",
      "expertise": ["Distributed Computing"],
      "endorsedBy": ["/organization/nlesc"],
      "involvedOrganization": ["/organization/nlesc"],
      "license": ["apache-2.0"],
      "logo": "/images/software/xenon.png",
      "name": "Xenon",
      "nlescWebsite": "https://www.esciencecenter.nl/technology/software/xenon",
      "owner": ["/organization/nlesc","/person/j.maassen"],
      "programmingLanguage": ["Java"],
      "startDate": "2013-04-12",
      "status": "active",
      "supportLevel": "specialized",
      "tagLine": "A middleware abstraction library that provides a simple programming interface to various compute and storage resources.",
      "technologyTag": ["Distributed","Library"],
      "usedIn": ["/project/emetabolomics","/project/simcity","/project/viaappia-patty","/project/esalsa","/project/amuse","/project/abcmuse","/project/biomarker","/project/computational-chemistry-made-easy","/project/large-scale-data-assimilation"],
      "user": ["/organization/nlesc","/person/j.maassen","/person/n.drost","/person/s.verhoeven","/person/j.borgdorff","/person/b.vanwerkhoven","/person/r.vannieuwpoort"],
      "website": "http://nlesc.github.io/Xenon/",
      "title": "Xenon",
      "slug": "xenon",
      
      "description": "<p>Xenon is a middleware abstraction library. It provides a simple\nprogramming interface to various pieces of software that can be used to\naccess distributed compute and storage resources.</p>\n\n<h1 id=\"why-xenon\">Why Xenon?</h1>\n\n<p>Xenon is developed by the Netherlands eScience Center as a support\nlibrary for our projects. Several projects develop end-user applications\nthat require access to distributed compute and storage resources. Xenon\nprovides a simple API to access those resources, allowing those\napplications to be developed more rapidly. The experience gained during\nthe development of these end-user applications is used to improve the\nXenon API and implementation.</p>\n"
    },
  {
      "@id": "http://software.esciencecenter.nl/software/xtas/",
      "id": "/software/xtas",
      "schema": "http://software.esciencecenter.nl/schema/software",
      "codeRepository": "https://github.com/NLeSC/xtas",
      "competence": ["Efficient Computing","Big Data Analytics"],
      "contactPerson": "/person/j.attema",
      "contributingOrganization": ["/organization/nlesc"],
      "contributor": ["/person/l.veen"],
      "discipline": ["Humanities & Social Sciences"],
      "documentationUrl": "http://nlesc.github.io/xtas/setup.html",
      "expertise": ["Text Mining","Distributed Computing","Information Retrieval"],
      "involvedOrganization": ["/organization/nlesc","/organization/uva"],
      "license": ["apache-2.0"],
      "name": "xtas",
      "endorsedBy": ["/organization/nlesc"],
      "nlescWebsite": "https://www.esciencecenter.nl/technology/software/xtas",
      "owner": ["/organization/nlesc","/organization/uva"],
      "usedIn": ["/project/candygene","/project/texcavator"],
      "programmingLanguage": ["Java","Python"],
      "startDate": "2013-01-01",
      "status": "active",
      "supportLevel": "basic",
      "tagLine": "the eXtensible Text Analysis Suite",
      "technologyTag": ["NER","NLP","Parsing","Sentiment analysis"],
      "user": ["/organization/nlesc","/organization/uva","/person/p.bos"],
      "website": "http://xtas.net/",
      "title": "Xtas",
      "slug": "xtas",
      
      "description": "<p>xtas is a collection of natural language processing and text mining tools, brought together in a single software package with built-in distributed computing and support for the Elasticsearch document store.</p>\n\n<p>xtas functionality consists partly of wrappers for existing packages, with automatic installation of software and data; and partly of custom-built modules coming out of research. Currently offered are various parsers for Dutch and English (Alpino, CoreNLP, Frog, Semafor), named entity recognizers (Frog, Stanford and custom-built ones), a temporal expression tagger (Heideltime) and a sentiment tagger based on SentiWords.</p>\n\n<p>A basic installation of xtas works like a Python module. Built-in package management and a simple, uniform interface take away the hassle of installing, configuring and using many existing NLP tools.</p>\n\n<p>xtas’s open architecture makes it possible to include custom code, run this in a distributed fashion and have it communicate with Elasticsearch to provide document storage and retrieval.</p>\n"
    }
  ]
